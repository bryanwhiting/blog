---
title: "Air Canada's failed AI-generated Refund Policy and other AI mishaps: When you need guardrails on your AI"
description: Be careful what your AI bot says. We can't yet contain today's AI - how do we expect to contain tomorrow's?
date: 2024-02-18 09:21:10
created: 2024-02-18 09:21:10
categories:
  - tech
  - ai mishaps
  - ai
  - ai guardrails
draft: false
---
In recent news, [Air Canada must honor refund policy invented by airlineâ€™s chatbot | Ars Technica](https://arstechnica.com/tech-policy/2024/02/air-canada-must-honor-refund-policy-invented-by-airlines-chatbot/).

![We can't even contain today's AI. How will we contain tomorrow's?](../img/dalle-guardrails-on-ai.jpeg){.preview-image}

Previously, trollers were able to convince the chatbot to agree to selling them a car for $1. [People buy brand-new Chevrolets for $1 from a ChatGPT chatbot](https://the-decoder.com/people-buy-brand-new-chevrolets-for-1-from-a-chatgpt-chatbot/). It's possible some of these things may have actually been legally binding if someone wanted to sue for it. 

And don't forget the lawywrs being sanctioned for not reviewing their AI: [New York lawyers sanctioned for using fake ChatGPT cases in legal brief | Reuters](https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/).

Be careful - AI is incredibly hard to control. Open tools exist to attempt impose guardrails, but they're not perfect: [Building Guardrails for Large Language Models](https://arxiv.org/html/2402.01822v1).

If we can't contain today's AI, can we contain tomorrow's? This isn't to be an alarmist, but a pragmatist. Machine learning was used in a lot of wrong ways before it was able to be harnessed for good. The same will likely be true for AI. 