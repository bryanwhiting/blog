{
  "hash": "ad335f6bf760a55512823070cc887303",
  "result": {
    "markdown": "---\ntitle: 'Buying a used car the data science way: Part 1'\ndescription: How I scrape used car data.\nauthor: Bryan Whiting\ndate: 2017-02-18\noutput:\n  distill::distill_article:\n    self_contained: false \ncategories: \n  - webscraping\n  - cars\n  - pricing\n  - r\n---\n\n\n\n\n*[Update 2021-11-16] This analysis was originally written on my old blog [here](https://htmlpreview.github.io/?https://github.com/bryanwhiting/blog_deprecated/blob/master/content/post/2017-10-18-getting-used-car-data.html). You can\nfind the source code for it [here](https://github.com/bryanwhiting/blog_deprecated/blob/master/content/post/2017-10-18-getting-used-car-data.Rmd). This code no longer works as TrueCar changed their CSS to make it more difficult to scrape. It's still possible, but you'd need to build a custom scraper from scratch.* \n\nThis is part 1 out of a two-part series on scraping used car data. Check out [part 2](../2017-10-19-used-cars-analysis/index.html) to learn how to analyze the data.\n\nIn another post, I describe how I use this data that I’ve scraped, but I wanted to provide a more in-depth tutorial for those interested in how I got the data. Note, this data belongs to Truecar, so all uses herein are for personal and academic reasons only.\n\n## Get the data\n\nIn order to do any good analaysis, you first need data. I prefer to have more data than less, where possible. In this case, I don’t have any data, so I use webscraping to get the data. There are much better tutorials on how to scrape data, so I’ll be light. I use R’s rvest package here, which does a decent job.1 Let’s look at Truecar’s Used Car postings2. First I use google to find the search query on Truecar that I like.\n\n## Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(magrittr)\n# Find the URL of the data you want to scrape\nurl <- 'https://www.truecar.com/used-cars-for-sale/listings/ford/edge/'\nread_html(url)\n```\n:::\n\n```\n## {xml_document}\n## <html lang=\"en-US\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset= ...\n## [2] <body>\\n    <!-- Element target for any partner code meant to execut ...\n```\n\nYou’ll see there’s a head and a body. Our data’s in the body, so let’s use html_nodes() and html_text() to parse out the data we want. I used Selectorgadget to know what HTML classes to search for.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_html(url) %>% html_nodes('.col-xs-6.col-sm-8.no-left-padding') %>% html_text()\n```\n:::\n\n```\n## character(0)\n```\nSo that’s how you get the data on a single page. If you look closer at the URL, you see a lot of helpful things. First, there’s the make, then the model, then the location-zip, then the year-range, and ultimately the trim. This is a very pretty and clean URL. If you click on a few additional pages, you’ll see the URL opens up with `?page=2`.\n\nhttps://www.truecar.com/used-cars-for-sale/listings/ford/edge/location-90210/?page=2\n\nThis is our ‘in’ to scraping multiple pages. I won’t bore you with the details of how to get that data into a neat matrix for us to analyze, but suffice it to say that I’m able to do it. Just build a function to construct a URL, and build a loop to go through the different pages, then use lots of `str_extract()` from the `stringr` package and `gsub` to clean up the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stringr)\n\nmake = 'ford'\nmodel = 'edge'\nzip = '90210'\nyear = 2012\nnpages = 5\n\nurl <- paste('https://www.truecar.com/used-cars-for-sale/listings/', \n             make, '/', \n             model ,\n             '/location-', zip,\n             '/year-',year,'-max/?page=', sep = \"\")\n\nurls <- paste(url, 1:npages, sep = \"\")\n\nscrape <- function(pageno){\n  try(\n    read_html(urls[pageno]) %>% html_nodes('.col-xs-6.col-sm-8.no-left-padding') %>% html_text()\n  )\n}\n\nlong_list = scrape(1)\nfor(i in 2:npages){\n  print(i)\n  new_list = try(scrape(i))\n  \n  error = (\"try-error\" %in% class(new_list))\n  \n  if( error == FALSE ){\n    long_list = c(long_list, new_list) \n  } else {\n    break\n  }\n}\n## [1] 2\nstats <- long_list\ndf <- as.data.frame(stats)\ndf$stats %<>% as.character()\ndf$price <- str_extract(df$stats, '\\\\$[0-9]*,[0-9]*') %>% \n  gsub('Price: |\\\\$|,', '', .) %>%\n  as.numeric()\ndf$year <- str_extract(df$stats, '^[0-9]* ') %>% \n  as.numeric()\ndf$mileage <- str_extract(df$stats, 'Mileage: [0-9]*,[0-9]*') %>% \n  gsub('Mileage: |,', '', .) %>%\n  as.numeric()\n\n# a = df$stats[1]\ndf$trim <- str_extract(df$stats, '.*Mileage:') %>% \n  gsub('FWD|AWD|4x[24]|[24]WD|V6|4-cyl|^[0-9][0-9][0-9][0-9]|4dr|Automatic|Manual|Mileage:', '', ., ignore.case = T) %>% \n  gsub(make, '', ., ignore.case = T) %>% \n  gsub(model, '', ., ignore.case = T) %>% \n  trimws() \n\n\ndf$awd <- grepl('AWD|4WD|4x4', df$stats, ignore.case = T) %>% as.numeric()\ndf$manual <- grepl('manual', df$stats) %>% as.numeric()\ndf$v6 <- grepl('V6', df$stats) %>% as.numeric()\ndf$location <- str_extract(df$stats, 'Location: .*Exterior:') %>% \n  gsub('Location: |Exterior:', '', .) %>% \n  trimws() \ndf$ext <- str_extract(df$stats, 'Exterior: .*Interior:') %>% \n  gsub('Interior:|Exterior:', '', .) %>% \n  trimws() \ndf$int <- str_extract(df$stats, 'Interior: .*VIN:') %>% \n  gsub('Interior: |VIN:', '', .) %>% \n  trimws() \ndf$vin <- str_extract(df$stats, 'VIN: .*\\\\$') %>% \n  gsub('VIN: |\\\\$', '', .) %>% \n  substr(., 1, 17)\ndf$deal <- str_extract(df$stats, '\\\\$[0-9]*,[0-9]* below') %>% \n  gsub('below|\\\\$|,', '', .) %>% trimws() %>%\n  as.numeric()\n```\n:::\n\n\nAnd here’s what the results look like. You’ve got the original scraped data in the stats column and then everything else that you can parse out. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# df was the dataframe object we needed\ndf %>% select(-stats) %>% head(10) %>% formattable::formattable()\n```\n:::\n",
    "supporting": [
      "used-cars-scrape_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}