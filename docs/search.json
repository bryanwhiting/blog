[
  {
    "objectID": "posts/2022-06-21-the-power-of-party-pay-by-visible/index.html",
    "href": "posts/2022-06-21-the-power-of-party-pay-by-visible/index.html",
    "title": "The Power of Party Pay by Visible",
    "section": "",
    "text": "Visible’s party pay madness\nVisible wireless offers $25 off to users who join a party. So a user who signs up to pay $50 for unlimited data can click one button to join a party and only have to pay $25/mo from then on. Why not just offer users $25 to begin with?\nIt’s really confusing to me as a user - why not just give me the $25 automatically if it takes nothing to join a group? Because of gamification.\n\n\nWhy it works\nI think their strategy is as follows:\n\nThey hope some people get brought in on marketing and never remember to activate a party pay. Let’s say this number is 20-40%. Perhaps these users fund the rest of the discounts.\nJoining a party pay group for free is an easy way to make your customers win. I.e., a user gets $25 off for joining. Gimmicky, but I think it works. So a user has an immediate sense of gratification - a great way to build a strong emotional bond. “Click a button to get $25 off in perpetuity? Nice!” Anyone would agree this feels better than just starting off with $25.\nThis makes users want to share and tell others. Especially if customers feel they’ve gamed the system. Who today doesn’t like a hack?\n\nBut even if (1) is wrong, and that 95% of people join a party I’m sure they’ve done the math to consider how many people they need to sign up to make the money work.\n\n\nThe Larger Strategy\nWhy would Verizon cannibalize their own sales? I switched my wife out of a $70 unlimited plan on Verizon to a $25 plan on visible for the same service. Well some customers will stay with Verizon because of its prestige and the add on benefits. But users who are cutting the cord are going somewhere else, I’m thinking Verizon wants to stay ahead of the curve. It’s a race to the bottom, sure. But it doesn’t hurt to try to win the race.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-28-pomodoro-principles/index.html",
    "href": "posts/2022-06-28-pomodoro-principles/index.html",
    "title": "Pomodoro Principles",
    "section": "",
    "text": "I’ve been trying a Pomodoro timer in TickTick and it’s changing how I approach my work day. I’ve been using it for two days so far and here are the differences I notice:\n\nWhen my mind knows I’ll get a break at 25 minutes, it allows me to focus more.\nIt’s easier to say not to slack messages or impulses to open the news if I know I’ll have time for those things later.\nA 5 min break is perfect.\nIf a distraction pops up, I write it down in my journal and tell myself I can follow up during my break.\n25 minutes is short. It’s amazing how time flies.\n\nI used to hate timing myself after working in consulting. I vowed I would never work on the clock ever again. But don’t the best athletes pace themselves? Any marathon trainer knows how to set a pace and have rest periods. Why wouldn’t the same principles apply to cognitive work?\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-28-increase-focus-through-writing-down-everything/index.html",
    "href": "posts/2022-06-28-increase-focus-through-writing-down-everything/index.html",
    "title": "Increase Focus through Writing Down Everything",
    "section": "",
    "text": "The GTD framework preaches something to the effect of “get every todo item down and into a system. It may take dozens or hundreds of tasks to get it all out.” Essentially, if a task is in your mind, it will slow you down. But if you put it down on paper, it will free your mind to think on other things. Also, more ideas will come.\nI’ve put perhaps a hundred things down on paper in the last few days. Interestingly, ideas I had forgotten about came back to my mind. I wrote these down.\nThere got to a point today though where I had so much on my screen I felt overwhelmed. How will I ever sort through all this mess?\nThankfully, most of the things don’t need to get done and are either “eventually” or “someday maybe” tasks, and not “asap” tasks. So just ignore them.\nAnd secondly, I always “know” what’s most important. This documentation process doesn’t change that. It just helps me to get all the distractions out of my head and down on paper.\nSpending time organizing all this stuff seems like a potential waste. But what I realize is that it improves my just-in-time recall. If earlier in the day I planned to buy a dish scrubber in the evening, when the evening rolls around that task magically comes back to my mind. So it’s really just front loading all the decision making and prioritizing. I think our brains are capable of more subconscious processing than we realize.\n(Pause entry to buy dish scrubber…) okay, I’m back.\nAnother idea that’s having an effect on me is the 2 min rule. If a task takes less than 2 min, don’t organize it. Just do it. Powerfully, this mindset helps me to care less about two min tasks. Or, sometimes there’s a tendency to over exaggerate the stress something will cause. Like buying a scrubber could be a 10 min task if you want to find the perfect one. But if you only allocate yourself 2 min, you realize this is only a $6 decision and I have better things to do with my time.\nLastly, I love being deliberate about tasks by writing down the intended outcome of a task. It’s very easy as a data scientist to get swallowed up in all the weeds. Or, rather, when your task is to dig up a gem it’s easy to get distracted by all the weeds that need to be pulled that are in your way. So by being clear about the reason I’m diving into the data in the first place helps me re-surface less scathed. Data are a dirty business…\n\nTakeaways\nThe effect of the GTD framework?\n\nLess FOMO. By writing down everything I can see clearly that 1) I don’t have time for all of it and 2) the stuff I care about pops up.\nI don’t have to worry about what I’m not doing because I can easily say no to it as I say yes to something better.\nI can see broader connections between a litany of things. This helps me prioritize what my subconscious has been telling me by nagging me about X ideas.\nIf I start working on something not in my list it makes me ask if this thing is fealty worth my time when I know I have a ton of other stuff to do. (Recency bias I spoke about in my other blog post.)\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-12-23-who-do-you-want-to-be-known-for/index.html",
    "href": "posts/2022-12-23-who-do-you-want-to-be-known-for/index.html",
    "title": "How do you want to be remembered?",
    "section": "",
    "text": "On the sports fields of high school, I was often asked, “How do you want to be remembered?” The energy behind that question is a rallying cry for someone to go out and score some goals and be victorious. Score goals → be remembered forever in the annals of high school lore, the thinking goes.\nThis question misses the point.\nIn “Ego is the Enemy”, Ryan Holiday asks the rhetorical question “to be or to do”. The mindset is that either we can focus on doing great work (spending time on action and education), or we can focus on being great (talking too much, dreaming too much). Ironically, greatness follows those who do, not those who say they will do.\nSo instead of asking “how will others remember me”, ask “what will I do for others?”\n\n\n\nDan Kiefer via Unsplash\n\n\nOn this Christmas season, remember that while Jesus Christ’s divinity has been debated for centuries, certainly he’s been remembered. He’s been remembered because of the actions he took and the kindness he showed.\nChristians believe he was the Son of God, one with literal power over death and rose from his own grave. He could calm the seas and turn water to wine. He was the rightful heir to the throne that his Roman captors sat on. He could have easily freed his oppressed nation to rest on his laurels forever.\nYet the recorded miracles we have from him don’t involve him showing moving mountains or collecting power and gold. The reason he’s remembered is because His miracles involve him spending time with individuals and in their service. He healed the sick. Caused the blind to see. He alleviated suffering.\nMay this Christmas be one where you don’t focus on what others think of you. May it be one where you enjoy following His example of doing good for others, however small.\n\nWhosoever will be great among you, let him be your minister; And whosoever will be chief among you, let him be your servant: Even as the Son of man came not to be ministered unto, but to minister, and to give his life a ransom for many. Matthew 20:26–28\n\nMerry Christmas.\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-07-03-when-data-collection-goes-too-far/index.html",
    "href": "posts/2022-07-03-when-data-collection-goes-too-far/index.html",
    "title": "When data collection goes too far",
    "section": "",
    "text": "Self Surveillance\nI came across this article in Flowing Data and thought it was fascinating.\nI’ve been on a productivity kick lately and notice these apps have habit trackers. I’ve tried using them in the past and have realized it gets too much too quickly. But that doesn’t mean as a data guy that I don’t think it’s a cool idea. It just overwhelms me.\nWe’ll I discovered that me tracking a few habits a day is junior varsity level. Felix’s blog is how it’s done to the extreme. Clearly this guy isn’t worried about Big Tech or Big Government harvesting his personal data. I love the boldness!\nThis is just 1/20th of the full page screen shot:\n\nThe number of slices and views he has is endless. It’s crazy to see everything he’s tracking, partly I’m sure because he has fun with it and the other part because he deep down hopes there’s something there.\nHis quote on Flowing Data has stuck with me:\n\nOverall, having spent a significant amount of time building this project, scaling it up to the size it’s at now, as well as analysing the data, the main conclusion is that it is not worth building your own solution, and investing this much time. When I first started building this project 3 years ago, I expected to learn way more surprising and interesting facts. There were some, and it’s super interesting to look through those graphs, however retrospectively, it did not justify the hundreds of hours I invested in this project.\n\nThis guy probably knows himself in a way that none of us ever will. He can’t ever lie to himself and say “yea, I’m good at exercising every day” because he’s got the data to back it up. But is this the life we want?\n\n\nSelf Surveillance as a Service\nSSaaS? Apparently. Exist.io tries to find patterns in your personal data and tell you when your most happy.\n\n\n\nExist.io\n\n\nWhen I researched the Apple Watch team a few years back I came to appreciate what they were/are trying to do: correlate heart monitoring with insurance claims data to proactively identify preemptive signs of cardiac arrest and other health issues.\nThat seems meaningful in theory because information about other people can be correlated with information about me.\nBut does info about myself correlate with info about myself? Probably. I have cycles. I have common traps and pitfalls like any human walking this earth. Some pitfalls are daily, some are quarterly or seasonal.\nBut I can also take 5 minutes to ponder reflectively or talk to a close friend and identify those patterns. What’s more human?\nAnd what’s more: if you end up changing your behavior then your past can’t really predict your future. So all of the data becomes moot once it achieves its purpose: true lasting change. Because if you truly change your behavior based on the insights you glean about yourself, those past triggers and anomalies no longer affect you in the way they did. So when you experience a personal step change in life, you’re a new you. Maybe not. But maybe.\nAlso, the new generations growing up on smartphones have a unique challenge: their online identities are given to them before they’re really conscious enough to determine if that’s what they want to show the world. Parents posting YouTube or TikToks of their kids are cementing for these kids an identity they themselves didn’t choose. We’ve all gone through phases of deleting Facebook photos from 15 years ago because it doesn’t reflect our current self image. What’s to be said of giving people an opportunity to change if everything they’ve ever said or done is recorded somewhere? Twitter. Instagram. TikTok. Facebook. Every post is cementing us in a way.\nI think it’s better to forget yesterday. Because it doesn’t have to have any bearing on where you’re going tomorrow. It can if you let it. But your past doesn’t have to define you.\n\n\nTakeaways\nWould you rather spend hours setting up digital automatic and manual tracking systems just to learn what you could by opening a gratitude journal every day? Sometimes less is more. I don’t think digital self surveillance is a direction we should go. (Not to mention the moment you do get hacked you’ll deeply regret it.)\nEven blogging can be dangerous for your career. Never know what’s gonna come back to haunt you years later.\nSo I should be careful what I say here, that is if I ever care to work for a self surveillance company in the future.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2020-12-18-productivity-notes/index.html",
    "href": "posts/2020-12-18-productivity-notes/index.html",
    "title": "Productivity notes",
    "section": "",
    "text": "Time Management\n\nCalendar my ToDos.\nShorter work is better. Less to edit. Less mistakes. Keep it simple.\n\n\n\nDistractions\n\nBlock 30 min for email and review team members work twice a day.\nAggressively filter email spam.\n\n\n\nAnalysis\n\nStart with a story. End with story.\nIterate quickly with stakeholders.\nalways have one nights sleep before sending a presentation. Edit.\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2018-07-02-semantic-versioning-for-data-science-models/semantic-versioning-for-data-science-models.html",
    "href": "posts/2018-07-02-semantic-versioning-for-data-science-models/semantic-versioning-for-data-science-models.html",
    "title": "Semantic Versioning for Data Science Models",
    "section": "",
    "text": "If you’ve ever wanted to tag your data science model, you’ve probably wondered how to version it. Which will it be: vx.4.1, v34.1231.51.21, or v91.x4.dev34? After reading about semantic versioning, I propose a method for versioning data science models.\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2018-07-02-semantic-versioning-for-data-science-models/semantic-versioning-for-data-science-models.html#an-example",
    "href": "posts/2018-07-02-semantic-versioning-for-data-science-models/semantic-versioning-for-data-science-models.html#an-example",
    "title": "Semantic Versioning for Data Science Models",
    "section": "An Example",
    "text": "An Example\nI build data science models by building python packages and committing the code to GitHub. The python package contains all of the support files and a main.py file to run the pipeline from start to finish. The support files could contain either helpful loggers to tell me where the pipeline is breaking down, diagnostic tools such as an AUC-ROC plot, files to engineer features, or files to train different model types. So there are really these three things that could happen to my code at any time. It doesn’t matter where those changes occur, or how many lines of code changed. What matters is what’s happened to the model. Each time I get a pull request approved, I’ll update the version number in my repo.\n\nChanging BETWEEN\nIncrement the BETWEEN version for the following changes:\n\nWhen the structure of your data changes\nIf your target variable changes (how you coded it, or data that produced the target changed)\nThe underlying population you’re training on changes\n\nAssume I discovered that I had a bad join. The join change might have been a minor one, but it’s now difficult to really compare the two models because my target was affected as a result. What I was predicting, even if slightly different, is now something different. If you ever feel like you’re comparing apples to oranges when looking at an AUC-ROC curve, update the BETWEEN version.\nWith each BETWEEN change, there should be a clear communication in the release notes of why the model is inherently different from prior models. These changes should be less frequent.\n\n\nChanging WITHIN\nIncrement the WITHIN number when the following happens:\n\nFeatures are added\nData sources are added or updated. (You might have several data sources today that help contribute to the target. But adding a data source might just mean adding new features. If you add data in such a way that it changes what you’re ultimately modeling, update the BETWEEN version.)\nNew modeling types are added. (You might be using a logistic regression, but add a mo)\n\nSo if I added files that allowed me to train different model types, or added files that engineered new features, I’d incrememnt the WITHIN version. In my mind, this shouldn’t be a count of features you include, but should just be incremented every time you do something that affects the model performance.\nAt any time, you might what to view different subsets of features and their effect on the model. So, you might not change the number of features, but add some functionality into your pipeline that now produces three different models in each run. I would increment the WITHIN version number in that case.\nRemember, the goal of this type of versioning is to show generally what’s changed in your pipeline.\n\n\nChanging PROCESS\nIncrement the PROCESS version when the following happens:\n\nA minor bug is fixed (unless this updates BETWEEN or WITHIN)\nEnhance a logger or aspect of the pipeline\nAdd a diagnostic plot or table\n\nBasically, any time you add something that doesn’t ultimately affect the performance of the model, you should update the PROCESS version. Your changes might improve the flow of your code and make you a much happier developer, but if it doesn’t improve the model, then the key priority of your code hasn’t improved."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html",
    "href": "posts/2022-11-12-what-is-a-model/index.html",
    "title": "What is a model?",
    "section": "",
    "text": "_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html#im-building-a-model-you-know",
    "href": "posts/2022-11-12-what-is-a-model/index.html#im-building-a-model-you-know",
    "title": "What is a model?",
    "section": "I’m building a model, you know",
    "text": "I’m building a model, you know\nThe first time I heard the word “model” was from an engineering professor in college. I had no idea what he meant so I asked him. He looked at me like many who have forgotten their own ignorance and just said “Some calculations in excel. You know.”\nNo, you don’t know. But nod your head so you don’t look like an idiot. “Right, of course.”\nWhy this moment was intriguing to me is one of my personal mysteries. But it was one of those odd moments that stuck. I’m now here to help my old self make sense of the world."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html#the-same-name-for-different-things",
    "href": "posts/2022-11-12-what-is-a-model/index.html#the-same-name-for-different-things",
    "title": "What is a model?",
    "section": "The Same Name for Different Things",
    "text": "The Same Name for Different Things\nWhen I told my mom I build models for a living she shook her head. Turns out my dad had been telling her for years that he builds financial models, yet she never figured out exactly what it was that he did.\nLet’s be clear on the common ground. A “model” per the dictionary is “simplified description, especially a mathematical one, of a system or process, to assist calculations and predictions.” By this definition, my dad and I do the same thing. We use numbers to put structure on the world."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html#types-of-models",
    "href": "posts/2022-11-12-what-is-a-model/index.html#types-of-models",
    "title": "What is a model?",
    "section": "Types of Models",
    "text": "Types of Models\nA financial model is different than a statistical model is different than a physical model or a 3D architecture model. But they all do the same thing: use numbers to put structure on the world.\nEvery model has a few things in common: - They make assumptions about the world - They use data - They probably make some calculations to help someone make a decision\nA financial model might make assumptions in a spreadsheet about how many customers you’ll get per month for the next 12 months. This doesn’t seem complicated, and it shouldn’t: models don’t have to be complicated.\nA data science model is one that might try to predict whether a customer is going to click on an ad."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html#how-does-a-model-work",
    "href": "posts/2022-11-12-what-is-a-model/index.html#how-does-a-model-work",
    "title": "What is a model?",
    "section": "How does a model work?",
    "text": "How does a model work?\nThe purpose of a math model is to imitate the “data-generating process”. Meaning, if you have some data on your customers (what they purchased and when), you would hope that there’s some rules that your customers are operating under. If you knew their income, their budget, their preferences, etc then you can guess how much they have to spend at a given time. Thus, if you know how much gas is in their tank, the mileage of the vehicle, the MPG of the vehicle, you can predict when they’ll stop to fill up on a road tip. When they fill up, they generate some data. “Bob purchased $34.52 of gas on Monday at 5:43pm.” Why did they purchase at that time of day? And why did they spend exactly $34.52?\nIf all you have outcome/results data then you can’t ever make predictions. But if you know what type of car Bob drives, how often he drives, how much gas he had in the morning when he left his house, etc, you can start to predict when he’ll next fill up.\nThe assumption behind a model is that what was true in the past will remain true in the future.\nWhy have there been so many issues because of COVID? Nobody could have predicted a world wide pandemic. The world wasn’t the same. All the models stopped working. Supply chain models. Financial models. Default models. All of these assumptions across all industries started breaking down."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html#how-do-you-build-a-data-science-model",
    "href": "posts/2022-11-12-what-is-a-model/index.html#how-do-you-build-a-data-science-model",
    "title": "What is a model?",
    "section": "How do you build a data science model?",
    "text": "How do you build a data science model?\nMost people build models every time they calculate the “mean”. This is the most ubiquitous model and anybody can use it. Sum your revenue and divide by number of purchases.\nIf you want to predict revenue for next Monday, just take the average revenue for every past Monday. That’s a model.\nIf Monday is a holiday or the Monday after Black Friday, your model might be very wrong. So you estimate black Friday’s sales by looking at last year’s sales. “Last Cyber Monday we sold 2x more than we typically do on a Monday. So I’ll predict that on this upcoming Cyber Monday I’ll do 2x more than what I did in the most recent Monday”\nNow that you’re taking in some data, your building a model. You’re translating a theory on how the world works into numbers. You’re taking last year’s sales and using it to predict tomorrow’s sales."
  },
  {
    "objectID": "posts/2022-11-12-what-is-a-model/index.html#start-simple",
    "href": "posts/2022-11-12-what-is-a-model/index.html#start-simple",
    "title": "What is a model?",
    "section": "Start Simple",
    "text": "Start Simple\nWhenever I build models, I always try to start simple and see if something more complicated can beat it.\n\nUse averages\nBuild a rules-based model\nBuild a regression model/machine learning model\nBuild a ML model with rules on top of it\n\nIf I’m trying to predict whether a user will click an ad, the simplest model is “what % of all users clicked this ad in the past?” If 20% of all users who see an ad click on it, then my “model” is “predict a 20% chance of click”.\nThen to make it more complicated, you can segment your users. “10% of men click on this but 25% of women do.” You now have more data and better predictions.\nA “decision tree” is a machine learning algorithm that just finds those segments in your data. “If man age 30 in Chicago then 22.3% chance of clicking based on all past behavior of all 30-year old men in Chicago”. It’s called a decision tree because it will put every user into mutually exclusive groups.\nSome “business logic” model might be making predictions around how your business operates. For example, if there’s a sale you may know that clicks increase."
  },
  {
    "objectID": "posts/2022-12-15-chatgpt-rshiny/index.html",
    "href": "posts/2022-12-15-chatgpt-rshiny/index.html",
    "title": "Building a complex R Shiny Dashboard Using ChatGPT",
    "section": "",
    "text": "ChatGPT is nothing short of amazing, but I think I’ll still have a job as a data scientist for at least another year or so.\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-12-15-chatgpt-rshiny/index.html#data-processing",
    "href": "posts/2022-12-15-chatgpt-rshiny/index.html#data-processing",
    "title": "Building a complex R Shiny Dashboard Using ChatGPT",
    "section": "Data Processing",
    "text": "Data Processing\nI asked it to create an indicator variable for me so that I could color my dygraphs chart by forecasts vs. historical:\n\nHere was my prompt:\n\nDon’t use add_series() since you’re wrong and it’s not a function in dygraphs. Instead, append the forecasts to the aapl in the stock_subset function and create a new indicator for the forecasted values where 0 is past data and 1 is forecasted data. Then, color the dygraphs chart based on the indicator value. Re-write this script below accordingly.\n\nLook at that code! It knew to filter appl2$ds &gt; max(appl$ds). That’s exactly how you subset a prophet dataframe (which returns all historical and future forecasts in one dataframe). It knew that appl was the first dataset and appl2 was the derived dataset. It knew that it had to take the max date from appl. This is absolutely amazing, hands down.\naapl2 &lt;- bind_rows(aapl2, forecast_subset)\naapl2$indicator &lt;- ifelse(aapl2$ds &gt; max(aapl$ds), 1, 0)"
  },
  {
    "objectID": "posts/2022-12-15-chatgpt-rshiny/index.html#data-inspection",
    "href": "posts/2022-12-15-chatgpt-rshiny/index.html#data-inspection",
    "title": "Building a complex R Shiny Dashboard Using ChatGPT",
    "section": "Data inspection",
    "text": "Data inspection\nI then got this error:\nI'm getting this error: Error in `select()`:\n! Can't subset columns that don't exist.\n✖ Column `date` doesn't exist.\nChatGPT gave me the ring-around. It didn’t know that the dates were stored as the rownames, but confidently gave me answers like this:\n\nTo it’s credit, it recommended I inspect the data myself:\n\nOnce I figured out the problem, I told it that the rownames were dates, and that date wasn’t a column. It had just the solution for me:"
  },
  {
    "objectID": "posts/2022-12-15-chatgpt-rshiny/index.html#misleading-functions",
    "href": "posts/2022-12-15-chatgpt-rshiny/index.html#misleading-functions",
    "title": "Building a complex R Shiny Dashboard Using ChatGPT",
    "section": "Misleading Functions",
    "text": "Misleading Functions\nTwice, it told me that dygraphs had functions that it didn’t actually have. It even wrote code for me. For example, it recommended this to me:\ndygraph(stock_subset(), main = \"AAPL Stock Price\") %&gt;%\n  add_series(data = forecast_subset, col = \"red\") %&gt;%\n  dyRangeSelector()\nAnd it doubled down:"
  },
  {
    "objectID": "posts/2022-12-15-chatgpt-rshiny/index.html#data-conversion",
    "href": "posts/2022-12-15-chatgpt-rshiny/index.html#data-conversion",
    "title": "Building a complex R Shiny Dashboard Using ChatGPT",
    "section": "Data Conversion",
    "text": "Data Conversion\nIt told me to use the xts package so that dygraphs could read the data in. This was accurate - dygraphs cannot read in tibbles.\n\nBut it was misleading me a little bit. Again, I had to look at the data, because the direct code it gave me was missing columns that it assumed it had."
  },
  {
    "objectID": "posts/2022-06-20-blogging-with-quarto/index.html",
    "href": "posts/2022-06-20-blogging-with-quarto/index.html",
    "title": "Blogging with Quarto, and why I don’t use Medium anymore",
    "section": "",
    "text": "Been here before, eh?\nI probably shouldn’t write this post considering I’ve also written posts “blogging with Jekyll”, “Blogdown”, and “distill” before. I’m flaky, what can I say. I’ve changed my blogging framework 7 times in 5 years (I started with blogger.com and Wordpress before Jekyll, and even used Medium for 25 posts.)\nBut it’s not my fault. There are too many static site generators and it seems that Rstudio is as flaky as me. Meaning, they built rmarkdown, created hugodown, blowdown, bookdown, and distill. Hugodown is a lightweight version of blogdown, and I never figured out why it exists. Then there’s workflowr and so many other great, random products out there.\nWhy quarto for me? It seems to have all the stuff that distill has, and then some. And given its design and collaboration with Wes McKinney (pandas guy) in building quarto, and given that Wes wrote his latest book with it, I think this will be here to stay for at least 5 years.\n\n\nWhy do I avoid Medium\nWhy don’t I blog with Medium? I wrote 25 posts there over 9 months and loved it. It makes writing and publishing incredibly easy, which is why I did it. With distill/blogdown the limiting factor was I’d have to be at my computer to get a post in. I love writing on my phone - I have over 2000 journal entries with Day One over seven years, several of which are “draft” posts for a blog that won’t see the light of day since I don’t care about them after I’ve written it. I want to be able to blog frequently, a short little blurb to keep up my writing, and medium enabled this.\nBut 1) I didn’t get many views (not that I care) and 2) my writing is stuck on their platform (confirmed to me this week as I’ve tried several old, broken tools that attempt to export posts out of medium).\nI got 50x the views on my blogdown site just from organic google searching. From reading other people’s posts about leaving Medium, it’s seems that most traffic to Medium posts comes from outside of Medium.\nSo while I’m certainly not into blogging for the money, I wouldn’t mind getting the content to people who it can help. And if medium didn’t enable this, what is its purpose?\nSo I’m back and finally decided that I’m going to own my own platform. Seth Godin wrote a blog about this topic once. Why be on Medium, substack, Facebook, etc or any other up an coming platform? We’ll they’re all skewed toward the 1% of people who make money on the platform. Everyone else is wasting their time. So might as well own your content. And better to focus on writing than waste time catching up on every platform.\nAlso, it’s fun! There’s a fuzzy feeling to blogging.\n\n\nFuture of quarto?\nI’m speculating here, but Quarto can be used withR Studio, Jupiter or VS Code. The last editor seems excitingly suspicious.\nPerhaps R Studio sees the writing on the wall that VS Code is taking over the IDE world and perhaps taking over the long term future of R Studio IDE. I personally don’t see why anyone would pay for R Studio when VS Code is free and has a better interface. And while R Studio has a few better integrations for R at the moment, it won’t once enough people see the light for VS Code and turn their contributions there. I’ve been using R on VSCode for two years now and it rivals R Studio in 95% of the feature set, but 100% has better window management.\nSo Quarto seems like a great opportunity for R Studio to begin monetizing on a publishing platform like R Studio connect. Platforms like Notion and Confluence are great for everyone except data scientists trying to share reproducible research. But a quarto server seems like a great way to finally bring in Python data scientists into the markdown-flavored documentation world. (Man, I personally can’t use Jupyter/Google colab notebooks when I can use R Markdown!) I’d heartedly welcome this prospect, though I have no idea what their plans for Quarto really are.\nTill next blog post about what blogging tool I’m going to use…\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-11-18-writing-a-tech-resume/index.html",
    "href": "posts/2022-11-18-writing-a-tech-resume/index.html",
    "title": "Writing a Tech Resume",
    "section": "",
    "text": "TL;DR: Use the “Accomplished [X] as measured by [Y] by doing [Z]” format.\n\n\n\n\n\n\nWriting resume is painstaking, but let me help jumpstart you. My resume below has gotten me dozens of interviews with top tech companies (Apple, Google, Facebook, Amazon, Microsoft, Waymo, LinkedIn, Hopper, etc).\nIt got me in the door, but let me be clear I didn’t pass most of those interviews 😝. That’s the point of the resume: open the door.\nLet me help you get in the door.\n\nMy template\nHere’s the google doc link to my resume template, also posted below.\nhttps://docs.google.com/document/d/1n8W79UBiWYhMWHeF4sNDtwaWxySRwU3t9isodbUuycI/view\nYou can copy this template and put in your own content.\nI designed and wrote my resume the way I did by following the guidance of these resources: \n\nBlog: Google’s Accomplished X as measured by Y by doing Z formula\nYouTube Video: Google’s How to write a resume\nBlog: Cracking the Coding Interview: how to write a resume\n\n\n\nCommentary\n\nIt takes some practice, but the XYZ formula is very powerful.\nEmboldening words can be helpful to people who have 3 minutes to scan your resume.\nI’ve never gotten commentary on how long my resume is or crammed. I’m guessing people just glance for a few keywords and some decent content and assume the rest of the resume is good.\nI’ve had a lot of discussions in phone calls about my Personal Projects sections. Fellow techies all have their own personal projects and they want to know about mine. It sends a signal you’re intrinsically motivated.\nWhile my resume is long-ish, I try to think: what story does this bullet point tell? What story am I trying to convey?\n\n\n\nHow can I Help?\nI hope this helps! Send me a message at bryanwhitingcoaching@gmail.com if I can help review a resume or give more tips.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-07-20-de-sciencing-data-science-and-talking-like-a-normal-person/index.html",
    "href": "posts/2022-07-20-de-sciencing-data-science-and-talking-like-a-normal-person/index.html",
    "title": "De-sciencing Data Science and Talking Like a Normal Person",
    "section": "",
    "text": "Building a Data Science culture\n\n\n\n\n\n\nNote\n\n\n\nTL;DR: Always keep it simple and always bring it back to the business need.\n\n\nA coworker once asked me for advice on how to bring technical rigor into a non-technical culture.\n\n\nWhen the Simple Solution Wins\nA few years back I wanted to see if I could predict which NBA team would win in their next match. FiveThirtyEight is a blog that does this every day. Could I build a model as good as theirs?\nI built a few models and put up a site. Below I have a table of accuracies for each of my models compared to FoveThirtyEight’s.\n\nWhat I learned is that Nate Silver’s FiveThirtyEight three models are only 66% accurate. In 20 hours I got a model as accurate (V02 above) as his using just team level data (win %, recent win streak, etc.) and no player data (injuries, etc.). Also my model was built using data from 2005-2017 to predict 2019 performance. His model was updated daily with recent stats.\nBut even more glaring is that if I made a prediction on a single feature (Home % &gt; Away %) I could achieve 63.7% accuracy. This feature is a 1 if the home teams win % is greater than the away teams win %, 0 otherwise. Meaning, the home team wins 63.7% of the time their season win % is higher than the away teams season win %.\nWas it worth building an XGBoost model with 30 features to get 65.8% accuracy when just a simple calculation is 63.7% accurate? Depends on business context and how valuable that 2% gain is and what the development/engineering cost of deploying a model is. But if I were you I’d start with the simple model to get it into production to start generating value and then come back later to optimize.\nCaveat: it took a lot of work to discover that feature. 90% of the problem was building the data set. So at the end, building a model was pretty simple once I had the data. (This is where cloud AutoML will take over going forward, sorry!.) And I needed the model to see the feature importance (this was the top feature.) This is a valid data science approach: use modeling to discover the patterns. But just because you have a model doesn’t mean it’s worth deploying.\nKeep it business focused: when building a recommendation system, start with “what’s most popular”. Why would you need K-means or whatever to begin? And can’t you build your own K-means by creating heuristic filters? For example: if you want to recommend me a shirt based on shirts I’ve searched, you can find all shirts of same color, price, and size and recommend those. Building a model to take into account dimensions, patterns, what others buy, etc is fine but should be a V2.\n\n\nMindsets\nThere’s a temptation to act like an academic professor and use fancy jargon when working with your coworkers. Even worse, there’s a temptation to think you’re smarter than your coworkers because you know this mathy thing that they don’t. But remember this quote:\n\nWhen they are learned they think they are wise…1\n1 2 Nephi 9:28\nThis pride power struggle is your downfall for two reasons:\n\nIt blinds you to thinking your solution is good just because someone else can’t understand it.\nIt blinds you from focusing on what matters most: getting the simplest and best solution.\n\nYour coworkers aren’t your students eating you. They’re your smart coworkers solving the same problems you are.\nA better mindset is that of a professional athlete, like a football player. They might be dumb as bricks at times but they’re all incredibly smart players. Can you explain your technical solution like a pro football player would?\n\nBad: “I built an XGBoost classification model that reduces the RMSE by minimizing the Gini coefficient on each node…”\n\nBetter: “I noticed that most users close the app after they reach this page. I tried looking at some reasons why and found a few common behaviors (DISCUSS, get feedback). I think we can predict when this will happen and here’s a framework to do that.”\n\nThere’s a temptation to think “I did all this technical work, I should get credit for it!” My manager once coached me, “Your stakeholders should be amazed by how simple you made it. Not by how complicated the problem is.”\nThe best example I know of this is Chris Voss, an FBI negotiator. Listen to his podcasts or YouTubes. He’s a humble guy and doesn’t get into the technical stuff. He knows the principles and keeps the explanations simple. He helps you feel smart when you listen to him. He’s one of the best negotiators in the world and if you were having dinner with him you’d think he was a normal guy. This is how data scientists should be.\n\n\nBugs\nThere are two types of bugs in data science: Software bugs and data bugs. You’ll spend 80% of your ramp up time on a project setting up your environment, learning the data, trying to understand the primary keys and logging and unique values, etc.\n\n\n\n\n\n\nTip\n\n\n\nNobody cares about how you do any of this (the same way you don’t care about how cereal ends up in your grocery store), but they do care that you do it because it shows that you’re thinking about the right things.\nThey’ll also worry that you’re spending too much time on the wrong thing, so make sure you believe your actions are solving a business purpose.\n\n\n\n\nTakeaways\n\nFocus on principles, not technical details.\nGet the simplest, useful solution into production first to start creating value. Then figure out if it’s worth optimizing. If your product gets deprecated in 3 months then the fancy model will be wasted time anyway.\nYour stakeholders should be amazed by how simple you made it. Not by how complicated the problem is.\nIf you can’t explain a trend with a simple bar/line chart, a fancy regression model won’t do.\nStart with the non-technical (basic math) solution. Then see if you can beat the basic math with a fancy model. Evaluate the cost/benefit of the technical solution relative to the dumb version.\nwhen you have a fancy solution, make sure you compare it to the basic solution. Make sure you can beat the solution your teammates come up with. And make sure you carefully lead them from “their” solution to yours. “At first I tried X. X was fine. But X didn’t account for Y. So I built a model M and saw that M out performs X by 10%.”\nJust because you know/use math, doesn’t mean it’s right for the business. Data Scientists can easily be myopic because the work we do is challenging and deep in the weeds.\nYour boss doesn’t care about the nature of the bugs, but you should try to bring first principles to the table so she can cover for you. “I keep hitting this bug in the data. Can we ask/work with Infra team to solve this because it eats my time.”\n\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-25-how-i-use-ticktick/index.html",
    "href": "posts/2022-06-25-how-i-use-ticktick/index.html",
    "title": "How I use TickTick",
    "section": "",
    "text": "I’ve recently read “Atomic Habits”, “Deep Work”, “The Power of Habit”, “Getting Things Done”, “Essentialism” and “Effortless”. These productivity paradigms influence this post.\nInspired by this Reddit post, I’ve tried thinking how I want apply the principles of GTD and these other paradigms. I think GTD is great for me because it helps me get ideas out of my head and Essentialism is a better pursuit to make sure I’m not caught up in minutia. I don’t see them as incongruent, but one as an ideation organization approach and the other as a relentless focus.\nFive steps of GTD:\n\nCapture\nClarify\nOrganize\nReview\nEngage\n\n\nCapture\nI have an iOS shortcut that lets me quickly add a TickTick task to my inbox. I put this on my Home Screen for instant access and to avoid distraction. I want to capture the idea, not open TickTick and get distracted by other ideas.\nOr I use the TickTick today widget and upload it to today directly. Depends on the task and how detailed I want to be.\n\n\nClarify\nHere’s the system I use to clarify.\nDefinitions:\n\nTask: an action item.\nProject: an intended outcome with multiple tasks.\nEpic: an intended outcome with multiple projects.\nKey Result: high level, long term accomplishment. (Increase revenue 10%)\nObjective: a squishy, feel-good purpose statement. “Deepen relationships with loved ones.”\n\nTask grooming process: Groom the inbox every night and identify the intended outcome and next action. Tag the task.\n\nIf the task was “send Birthday card”, that’s really an intended outcome and is a small project. The tasks are 1) buy card from CVS, 2) put in envelope with stamp, 3) drop in mailbox. These three subtasks need to be understood. But a separate inbox entry may be “pay phone bill by sitting down at my computer at 10pm”. This isn’t a project. It’s just a one off thing. But it can still be clarified with details.\nLabel if it’s a project or not. Use iOS shortcuts to make this really easy. For example, gg = “🎯 #Project”\nThe challenge is if I should have things as projects or as individual tasks. I would say that as much as things can be independent, keep them tasks. Like “apply for credit card1” and “apply for credit card 2” are both personal finance tasks. But they are independent.\nA task isn’t necessarily short and a project isn’t necessarily long. It’s just whether this Indy ended outcome has multiple steps or not that might be done over a few days, whereas a task is probably done in one sitting.\nIt’s assumed that unless something is a task, it’s a project.\nEvery epic has its own name. I nest epics under “Work &gt; #Epic1, #Epic2”, etc. This helps me organize my work deliverables. I have like 5 epics right now at work that will span several quarters.\n\nThen I may choose to add tags with metadata, which I haven’t figured out if this is useful yet. But “Atomic Habits” says the best way to get stuff done is to stack it. So here is my tagging system to help me stack:\n\nCommitment: ASAP, Eventually, Someday Maybe. Will I do this task eventually? Pay a bill - yes. Learn a language - someday maybe.\n\nDuration: 5min, 30min, 1hr, 2hr, 4hr. 9hr (all day)\nLocation (where to do the task): Home, Desk, Car, Backyard. Most work things are done at my desk. But perhaps some can be done while taking a walk. Most personal things are done around the house or on the couch at night.\nTime of Day: “1: Pre work”. “2: start of workday. 3: lunchtime, 4: afternoon, 5: after work, 6: nighttime”\nFocus: “Deepwork” or “shallow”. Deep work should be uninterrupted and be 3 hours long.\nSeverity: S1: if I don’t do this nothing will happen. S3: not doing this will have bad consequences or lots of regret. S2 - in between :).\nDeadline: I’m still thinking about this one. To me there’s a difference between mowing the law which has a linear cost of looking worse each day for not being cut and failing to pay my taxes, which has festeringly worse consequences the longer I put it off (pay a fine, end up in jail, etc.). I guess this is captured implicitly in “ASAP” commitment. But paying my taxes is only a cost (preventing negative outcomes) versus the ASAP of getting my project finished (positive consequences).\n\nNone (default): no consequence for never doing this.\nBinary: You either make the deadline or never and this opportunity ceases forever.\nRolling: If you miss the deadline there will be another.\nFestering: the longer you put this off the worse it becomes, but no deadline.\n\n\nAs I write this, this sounds like overkill…classic me. I sincerely doubt it’s useful to of all of this. But having the system available will let me use what I need when I need it. If I’m still using any of this in a month that’ll be a miracle. But I’m going to start with something in mind and go from there.\n\n\nOrganize\n\nOnce clarified, move out of inbox into a list “work” or “personal”.1\nPrioritize.\nDelegate. (Myself, someone else.)\nAssign due dates and times (if not already done)\n\n1 Tags and lists are identical except Lists have sections and views (such as Kanban). In a list you can sort by tag and vice versa. But each task can only appear in one list whereas tags are infinite. I noticed that choosing which list something belongs to can be cumbersome if it’s in a nested list, so I want only high level lists (no folders) and have this be simple. I also have “personal” and “work” tags which have projects within them, so I don’t really see why I’d need to duplicate this. On second thought, each task can only belong to one work project so it’d make sense for this to be my grouping. I suppose I just don’t want to manage all that. Or, I chose to use tags to manage my projects because I was cheap and I can only have 9 lists on the free version but unlimited tags? Either way this is easy to reorganize later if I choose to switch. Tags it is for now.\n\nReview\n\nEach night I will review the days tasks and re-prioritize as needed.\nPin the one task/project I want to get done that day.\n\nEssentialism says to take the 6 tasks of the day and choose just one. Then identify what else you need to say no to to ensure you have the energy to accomplish that task.\n\nWeekly planning session to review all of this.\n\n\n\nEngage\n\nDon’t work on any tasks in a day/week that weren’t planned for that day (unless something comes up).\n\n\n\nFinal Thoughts\nA task system should be saving me time, not consuming it. It should be relaxing me and my anxieties, not amplifying them. I’m going to give this 30 days and see if my stress decreases and productivity increase. If so, I’ll keep using this system.\nGTD states that by not writing down all the ideas, you waste energy because things come back to nag you later. But surely energy is wasted by organizing a valueless idea.\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-12-06-more-new-businesses-than-ever-/index.html",
    "href": "posts/2022-12-06-more-new-businesses-than-ever-/index.html",
    "title": "Models as a Service and the Future of Data Science",
    "section": "",
    "text": "Businesses are Launching Like Crazy\nThere’s never been a better time to start a business. The Census shows there are over 200,000 new business applications per month more than pre pandemic.\n\nWhat’s driving this?\nListening to The Unintended Consequences of Working from Home it dawned on me that there’s a hyper acceleration of innovation happening.\nConsider the rise of AI tools like ChatGPT and StableDiffusion. Chamath Palihapitiya discusses the expansive applications for these services. What was once SaaS will become MaaS (models as a service). There will be models for everything: copy writing, code debugging, converting wireframes into working apps, ad generation, book writing…\nWhile tools like ChatGPT are just in demo mode, it’s wildly apparent they’ll be an impetus for new ideas and reducing the friction of getting to the answer. For example, as a data scientist if I need boilerplate code to clean some data, I can ask ChatGPT for the 80% solution. I’m already spending 30% of my day copying code snippets from StackOverflow and making it work for my problem. ChatGPT will just accelerate that process. 10 years from now if Neuralink can read my thoughts I don’t even need a keyboard to do this…we’ll build things at the speed of thought.\nConsider this for writers and marketers. They’ll use ChatGPT to unblock them, generate ideas, and get started. Innovation across the board will accelerate because the kinetic friction to getting started will reduce to zero. You can then spend your time on the editing and refining.\n\n\nThe Best Time to Start a Business\nI digress. Why is now the best time to build a software business?\n\nCompute is cheap. Gone are the days of having to build a server to prototype and idea.\nBarriers to prototyping are nearly free and scaling with the cloud is nearly infinite. Consider how the Gas App can launch to 100,000 new users per day with just a few developers.\nModeling is now easy. All cloud services have their own AutoML tools to build machine learning models for you.\n\nThe center of all of this is scalable data infrastructure and AI.\n\n\nFuture of Data Science\nThe data scientist of today may still be using sklearn to manually do hyperparameter tuning, but the data scientist of tomorrow will either be exclusively building MaaS B2B products or be the one person in their organization who will leverage all the MaaS tools.\nIf we can ask ChatGPT for complex answers it learned from the internet, why couldn’t something similar be trained on your company’s internal data? If that’s possible, then why couldn’t you just have a Q&A session with your internal ChatGPT? Consider the following example:\n\n“Hey KITT, what are the user segments generating the most revenue on Tuesdays at 5pm?” (Analytics)\n“How many sales do we anticipate from these users next Tuesday?” (Machine Learning)\n“What ads have been most effective for us in driving new sales in the past 7 days?” (Statistics)\n“Okay, KITT, generate 3 variants of that ad for next Tuesday” (StableDiffusion generating a new Ad)\n“I like ads A, B and C. launch an experiment on Facebook/TikTok, run it for three days and dynamically allocate ad spend to the best ad.” (Business integrations, statistics, ML models)\n\nThe data scientist of the future will be the one with deep understanding of these tools and an awareness of their biases. It will be very difficult to know when they’re lying to you. The in-house data scientist will be keen on knowing the biases in the data fed into these algorithms and how to make judgments on where to go next.\nEarly stage companies won’t need data scientists because most of this stuff will be off the shelf and easy for an engineer to integrate. That means that the data scientists will join these MaaS companies to build out optimized products that solve these niche problems.\n\n\nTakeaways\nData Scientists shouldn’t be scared their jobs will disappear. They should be excited that they’ll be joining companies that automate what they’re currently doing and offer it as a service.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent Posts",
    "section": "",
    "text": "“All blogs are wrong, but some are useful.”\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nScientific Freedom\n\n\n\n\n\n\nbook review\n\n\n\nInnovation comes by varied incentives: intellectual curiosity and economic incentive \n\n\n\n\n\nTue Dec 12, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nFuture of Coding\n\n\n\n\n\n\ndata science\n\n\n\nAs our jobs are automatable, we need to discover what’s not \n\n\n\n\n\nTue Dec 12, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow do you want to be remembered?\n\n\n\n\n\n\nleadership\n\n\nservice\n\n\n\nIt’s a question as old as time, but misses the mark and is distracting from the more important question. \n\n\n\n\n\nFri Dec 23, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a complex R Shiny Dashboard Using ChatGPT\n\n\n\n\n\n\ndata science\n\n\nr\n\n\nshiny\n\n\nfuturism\n\n\nchatgpt\n\n\n\nChatGPT can create amazing boilerplate code and can point you in the right direction, but it cannot debug data problems easily (yet). It can read in data from the internet, make time-series forecasts, and visualize the data (with a little assistance). \n\n\n\n\n\nThu Dec 15, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nModels as a Service and the Future of Data Science\n\n\n\n\n\n\ntech\n\n\nstartups\n\n\nbusiness\n\n\ndata science\n\n\n\nData Scientists shouldn’t be scared their jobs will disappear. They should be excited that they’ll be joining companies that automate what they’re currently doing and offer it as a service. \n\n\n\n\n\nTue Dec 6, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nWriting a Tech Resume\n\n\n\n\n\n\ncommunication\n\n\n\nHow to write a resume to get into a FAANG company. \n\n\n\n\n\nFri Nov 18, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nResume Substance over Style\n\n\n\n\n\n\ncommunication\n\n\n\nThe content of your resume is more important than the design. \n\n\n\n\n\nFri Nov 18, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a model?\n\n\n\n\n\n\ndata science\n\n\n\nPutting the mysterious in context. \n\n\n\n\n\nSat Nov 12, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nDe-sciencing Data Science and Talking Like a Normal Person\n\n\n\n\n\n\ndata science\n\n\ncommunication\n\n\nprinciples\n\n\n\nHow can data scientists bring their technical knowledge to a non-technical audience? Here are my lessons learned from seven years in the data trenches. \n\n\n\n\n\nWed Jul 20, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhen data collection goes too far\n\n\n\n\n\n\nproductivity\n\n\nfuturism\n\n\ntech\n\n\nsurveillance\n\n\n\nIt’s important for us to track certain things about ourselves. But this is clearly not exactly healthy IMO. \n\n\n\n\n\nSun Jul 3, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nPomodoro Principles\n\n\n\n\n\n\nproductivity\n\n\nticktick\n\n\n\nThe art of focus through 25min work blocks. \n\n\n\n\n\nTue Jun 28, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nIncrease Focus through Writing Down Everything\n\n\n\n\n\n\nproductivity\n\n\nmusings\n\n\nticktick\n\n\n\nTo be more present, get it all down and into a system. \n\n\n\n\n\nTue Jun 28, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow I use TickTick\n\n\n\n\n\n\nproductivity\n\n\n\nAn overview of how I apply the principles of “Getting Things Done”, or GTD. \n\n\n\n\n\nSat Jun 25, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Things Done\n\n\n\n\n\n\nbook review\n\n\nproductivity\n\n\n\nA powerful system to be more present. \n\n\n\n\n\nFri Jun 24, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the best todo list manager?\n\n\n\n\n\n\nproductivity\n\n\napp review\n\n\n\nI’ve reviewed 20+ apps. I think I’m done reviewing them. \n\n\n\n\n\nThu Jun 23, 2022\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nNew ideas are distractingly exciting\n\n\n\n\n\n\nproductivity\n\n\npriorities\n\n\nideation\n\n\n\nBe careful of recency bias with your ideas. \n\n\n\n\n\nThu Jun 23, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nDownstream Impact\n\n\n\n\n\n\ndata science\n\n\nexperimentation\n\n\n\nWhen you do an online experiment, you’re changing the future forever. \n\n\n\n\n\nWed Jun 22, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Power of Party Pay by Visible\n\n\n\n\n\n\nstartups\n\n\nstrategy\n\n\n\nVisible wireless, owned by Verizon, offers users $25 off per month if they join a meaningless party. What’s the effect? \n\n\n\n\n\nTue Jun 21, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nProducts for dads\n\n\n\n\n\n\nproductivity\n\n\nfatherhood\n\n\n\nWhat products am I still using 5 years later? \n\n\n\n\n\nTue Jun 21, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging with Quarto, and why I don’t use Medium anymore\n\n\n\n\n\n\ndata science\n\n\n\nI’m trying out quarto and I like it. \n\n\n\n\n\nMon Jun 20, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Author’s Angle Matters\n\n\n\n\n\n\ncommunication\n\n\nwriting\n\n\n\n\n\n\n\n\n\nSat Jul 17, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nProductivity notes\n\n\n\n\n\n\ndata science\n\n\nproductivity\n\n\n\nRunning list of productivity ideas. \n\n\n\n\n\nSat Dec 19, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to mock the value of an ML solution\n\n\n\n\n\n\ndata science\n\n\ninnovation\n\n\n\nSell your solution before training your model. \n\n\n\n\n\nWed Dec 16, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nGet rid of the todo list. Calendar everything.\n\n\n\n\n\n\nproductivity\n\n\n\nIf you’re not willing to block time for it, does it event matter? \n\n\n\n\n\nTue Dec 15, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nData Beats Opinions\n\n\n\n\n\n\ndata science\n\n\ndecisions\n\n\n\nOpinions from the customer shape the product. Their behavior (captured as data) speaks louder than their words. \n\n\n\n\n\nTue Dec 15, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSemantic Versioning for Data Science Models\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\n\nMeaningful versioning for data science models and machine learning pipelines. \n\n\n\n\n\nMon Jul 2, 2018\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuying a used car the data science way: Part 2\n\n\n\n\n\n\nwebscraping\n\n\nr\n\n\nregression\n\n\nanalysis\n\n\npricing\n\n\nbuying things\n\n\n\nHow I analyze used car data to find under-valued cars, and why none of my analysis matters.\n\n\n\n\n\nSun Feb 19, 2017\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuying a used car the data science way: Part 1\n\n\n\n\n\n\nwebscraping\n\n\ncars\n\n\npricing\n\n\nr\n\n\n\nHow I scrape used car data.\n\n\n\n\n\nSat Feb 18, 2017\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe DataViz battle: Plotly vs ggplot2\n\n\n\n\n\n\ndataviz\n\n\nr\n\n\nggplot\n\n\nplotly\n\n\n\nWhat can you do with plotly vs. ggplot2 and how do they compare on a simple chart?\n\n\n\n\n\nFri Feb 10, 2017\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarting with Jekyll\n\n\n\n\n\n\nproductivity\n\n\nblogging\n\n\njekyll\n\n\n\nHello world.\n\n\n\n\n\nThu Nov 17, 2016\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n_________________________  For coaching on data analytics or machine learning, reach out.",
    "crumbs": [
      "Recent Posts"
    ]
  },
  {
    "objectID": "book-reviews/2022-06-24-getting-things-done/index.html",
    "href": "book-reviews/2022-06-24-getting-things-done/index.html",
    "title": "Getting Things Done",
    "section": "",
    "text": "Principles and Core Insights\nAfter 21% my takeaways are:\n\n“My mind is weighed down by all of the ideas of things I can do.” I waste my time on todos that are just “go to doctor”, but should be “schedule appointment to go to doctor”. Anything that only takes 2 min should be done immediately. Anything longer later. But I love that clarity - I’ve often written lots of todos but not from the “next action to take” perspective.\n“Visions can be built from the ground up rather than just top down.” I’ve tried writing down big visions for my 10 year future and it never works for me. I’ve got anxiety when I try to think where I want to be in 10 years. I mean, I know I want to be happy, be best friends with my wife and kids and have meaningful relationships but whether I’m still a data scientist or a farmer by then is anyones guess. (I wanna keep my options open.) But as I look at the things on my todo list and “eventual” to do list, a different picture emerges. I know I want to be a writer (hence this blog), I know I want to travel to XYZ location, to be involved in giving back, etc. TLDR: I know things I want to experience in life. I don’t have to craft value statements and invent ideas of what to do (seems forced). Instead I can reflect on what’s currently exciting and important to me and categorize those things into high levels values/missions. This will help me when I need to prioritize so not every whim of an idea gets my attention.\nGTD book just lays out what a todo is. It breaks down the problem of doing things (big and small) into what they are (time sensitive, someday, etc). Whether I’ll follow the system is up for debate, but I love the ideas so far. It very much fits my naturally organized mind, but may not work for more spontaneous folks that don’t like as much structure and list keeping.\nMost of my “todos” have been “intended outcomes”, but not “next actions”. For example, I see I have “change car battery” on my list right now and “plan yosemite trip”. But these aren’t todos. I can’t organize and prioritize outcomes. I can only organize and prioritize actions, he says. Changing these to “call mechanic to book oil and battery change and identify an hour in your calendar”. And “open recreation.gov when walking around the house and find a campsite.”\nOrganize tasks around where you’ll do them. This is in line with Atomic habits which states to be hyper specific about where and when you’ll do something. “Call grandma when I get into my car tomorrow morning to go to work.” Or “take a moment at my desk during lunchtime to find a mechanic by opening Google Maps and plan an oil change. Find 1 open hour on the calendar.”\n\n\n\nPutting it into practice\nI’ve been using TickTick for my task manager, which has Lists (folders) where I can put my tasks and tags so I can label and categorize each task. Here’s my organization strategy:\n\nWhen jotting ideas write intended outcome and next action as my todo.\nTag with where I’ll do it and provide day or time it’s due (if applicable)\n\nMove each item to a respective list:\n\nASAP - there’s a useful urgency to this task (begin memo)\nEventually - there’s no urgency but I know I’ll want to do this\nSomeday maybe - no urgency and no commitment, but it’s an idea of something I want to do.\n\nTagging system: tag each item with either Work or Personal. Then within that, some meaningful themes:\n\nProjectName (work deliverable or personal project like “ImproveMyHealth”, “BuildApp”)1\nLocation (where I’ll do it). Around the house. At desk. In car. While cleaning the house, etc. this is habit stacking. Most work stuff is done at my desk.\nwhen I’ll do it. Stack with other activities. Examples: Before lunch.\nDuration (how long it takes: 1, 2, 4 hours, 5, 30min). This might be useful for planning.\n\nList system. Move from inbox to list when properly categorized. I like “Work” and “Personal” lists. I think since you can use unlimited tags, and tags can be nested in TickTick, there’s no substantive differ ne between list and tag. Not sure yet how to use the TickTick lists when tags are so useful. Lists of work and personal.\n\n1 Deciding Vision: As I write this, choosing a project might be a good place to insert a goal (intended outcome) as a project name. Instead of Exercise, the intended outcome is improving health. The goal is being able to run a 5k. So perhaps goal/objective is “ImproveHealth” and KR is “run 5k”. These goals can be identified by asking the 5 whys. If I have an idea if “go running” as my task, a good grounds-up vision exercise would be to ask “why do I want to run a mile tomorrow? Is it to accomplish something or to relax my mind? Why is relaxing my mind important to me? Why is my mental health important? Etc. I think subconsciously ideas come of what to do next all the time “I should work out” but rarely do I take the time to ask “why do I see the need to work out? What’s the greater vision here for this task?” Understanding that vision can help prioritize and build a desire to accomplish the task. If the idea “learn a new language” comes up, asking why can help me realize “oh, this is just for fun and won’t be for anything meatier than having fun! No pressure on this task and maybe I’ll never do it.” The nag to run a mile might help me realize “I need to take care of my health, and this can’t wait.”In summary:\n\nIdea/task comes.\nDocument in inbox.\nClarify the intended outcome and the next action.\nOrganize: tag. Set up reminder. Move out of inbox to list.\nReview and Prioritize: understand where these ideas will take you and choose your adventure.\nDo!\n\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "book-reviews/2023-12-12-scientific-freedom/index.html",
    "href": "book-reviews/2023-12-12-scientific-freedom/index.html",
    "title": "Scientific Freedom",
    "section": "",
    "text": "Max Plank, GPT 4\n\n\n\n“Scientific Freedom”\n\n\n\n\n\n\nTL;DR: a scientist isn’t a title, but a philosophy\n\n\n\n\n\n\nScientists are hampered by the peer review process. The notion that some tolerate the process so they can get their grants approved and get tenure, and others eventually accept the process because that’s how they got tenure, is a sad outcome of human incentives.\nInterestingly, inventions have always happened regardless of peer review. Edison didn’t need someone to review his invention to tell him whether or not it’s a valid invention. Either the light bulb illuminates or it doesn’t.\nYet that’s exactly how things go now. So what’s the result?\nPeople write to please the peer reviewer. Social conformity. Group think.\nPeople are incentivized to game the system: write a grant that gets them funding so they can get tenure and support their families.\nThe incentive to be innovative is small, though intellectual curiosity powers through for some. The incentive to stay in line and do what’s necessary to get approved is strong.\nWe’ve also learned as a consequence of 2020-2022 that a lot of scientists can indeed be wrong. Today science is becoming less about discovering truth and more about people owning their truth…people love their politics.\nWhat makes a scientist anyway? Am I a scientist because I’m a data scientist? Because I studied statistics? I’ve never published an academic article in a journal, so is that the threshold?\nI think someone who pursues truth through repeatable experimentation is a scientist. Back in Plato’s day they were just called philosophers.\n\n\nToday’s incentives\nThere’s a reason really smart people now work at Apple to invent the iPhone, Facebook to work on LLMs, and SpaceX to build rockets. The freedom they feel at these companies is empowering. Per “Scientific Freedom”, if the trends continue then people like Max Plank would never have been able to get the funding needed to invent what he did. Where’s the money? Social media. Advertising. And now, AI.\nPeople will always be inventive. But it’s by removing the social barriers that they become so.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Last updated: 2022-12-14\n_________________________  For coaching on data analytics or machine learning, reach out.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "Bio",
    "text": "Bio\nData scientist with 7 years of experience building complex, large-scale data pipelines and inferential and machine learning models. Experience running online A/B experiments, managing complex compute infrastructure and confidently presenting to VP-level audiences. Passionate about innovation, creativity, tech, data, engineering, leadership, mentoring and team building. Former startup co-founder.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nSee  LinkedIn or Resume for details.\n\n\n\n\n\n\n\n\n\n\n\nPresent\n\n\n\n\n2022/01\nHopper\n\n\n\n\nRemote\nSenior Data Scientist, Hotels Marketplace\n\nApp Experience: recommendation systems, A/B testing, product insights, user analysis. Price Merchandising: pricing and markup optimization, competitive pricing intelligence.\n\n\n2022/01\n\n\n\n\n2020/03\nGoogle: Maps, YouTube\n\n\n\n\nSan Bruno, CA\nData Scientist, Engineering\n\nGoogle Maps: A/B testing, product insights, user analysis. YouTube Music: A/B testing on music recommendation system enhancements, user insights, marketing analytics, user-targeting and churn analysis\n\n\n2020/03\n\n\n\n\n2017/07\nCapital One\n\n\n\n\nWashington, D.C.\nData Scientist, Machine Learning\n\nMachine learning and ML data pipeline development, cloud compute (AWS), risk modeling\n\n\n2017/07\n\n\n\n\n2015/08\nBates White Economic Consulting\n\n\n\n\nWashington, D.C.\nAnalytics Consultant\n\nEconometric and financial modeling in the context of antitrust litigation\n\n\n2014/08\n\n\n\n\n2013/01\nNovi Security\n\n\n\n\nProvo, UT\nCo-founder\n\nInnovative, wireless home security. Top 2% Kickstarter. Successfully raised seed round of investment",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#education-and-service",
    "href": "about.html#education-and-service",
    "title": "About",
    "section": "Education and Service",
    "text": "Education and Service\n\nM.S. in Statistics, 2013/08 - 2015/05,  Brigham Young University, Provo, UT\nB.S. in Statistics, 2008/08 - 2015/05,  Brigham Young University, Provo, UT\nFull-time Humanitarian Service, 2009/08 - 2011/08,  Tegucigalpa, Honduras",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/2021-07-17-authors-angle-matters/index.html",
    "href": "posts/2021-07-17-authors-angle-matters/index.html",
    "title": "The Author’s Angle Matters",
    "section": "",
    "text": "Imagine the following story:\n\n“In recent events, a ferocious fox savagely attacked a Turkey, who fortunately was able to escape to safety by pecking out the fox’s eyes.”\n\nSimple story. A bit gruesome. But, what in today’s news cycle is positive? How can we be better readers of the news when it’s targeted at making us angry?\nFoxes and turkeys are highlighted in the news all the time and it’s often the worst of humanity. The goal of many authors is to evict emotion from you so you’ll keep reading. Anger. Fear. Worry. Those keep you hooked.\nWhat’s the author’s motive? Often we have to guess it. Wouldn’t it be nice instead if authors were transparent about why they were writing a story:\n\nAuthor 1: I’m a Turkey conservationist with a passion for protecting turkeys from the rampant fox problem we have. Growing up, a Turkey saved my life when I was drowning in a lake.\nAuthor 2: I’m a narcissist who hates foxes because one time they ate my cat. I really just need therapy, but I can’t fathom talking to anyone about my problems. I want you to be as angry as I am.\nAuthor 3: I’m here to get promoted and if my boss sees that I have a 10% increase in viewership then our company’s bottom line increases. Im an opportunist who will say anything and take any side that will maximize company profits. How do these three perspectives change the storyline?\n\nWouldn’t it be great if we could know the author in addition to what the author is writing about? Every article should have not just “about the author” but, “three people’s perspectives about the author’s intent in writing this that know the author well.” Of course, you’d need to verify the three verifiers, but it’d be a start\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2016-11-07-starting-with-jekyll/starting-with-jekyll.html",
    "href": "posts/2016-11-07-starting-with-jekyll/starting-with-jekyll.html",
    "title": "Starting with Jekyll",
    "section": "",
    "text": "Hello world.\nI wanted to start a blog. I wanted to set it up for free, use a custom URL (and not something.bloggingplatform.com), and be able to both blog and create tutorials. I didn’t mind it possibly being technical1. Enter Jekyll.\n1 Specifically, I was interested in being able to use the command line for my posts. Additionally, I didn’t mind seeing or using a little bit of code.If you want to get your blog in 10 minutes, skip to below.\n\nWhy Blog with Jekyll?\nHere’s why you can and should blog with Jekyll (if you’re a data scientist):\n\nJekyll has beautiful, free, open-source templates.\nGitHub will host your website for free2.\nJekyll is static.\nJekyll can be simple to set up3.\nI tried Wordpress and blogger.com and got frustrated.\nThe blogging content you create is very portable (easy to take your posts from one blog site to another).\nCreating blogs in Markdown syntax is a dream (even for non-technical folks).\nYou can manage your blog from your terminal\nYou can manage your blog from your phone (if you have a GitHub-editing app)\n\n2 Someone has to host it (i.e., store and display all the files). I started off trying to set up a WordPress blog, but you have to pay another company to host. I didn’t want to bother.3 If you try to set it up by following the instructions on jekyllrb.com, that’s the complicated way. That’s how I originally started, got lost for five hours, and stopped. Six months later, I found a better way.But if you don’t like debugging occasional errors, you probably shouldn’t blog with Jekyll.\nWhat is Jekyll, you ask? Jekyll is an open-source blogging platform. Anyone can write a template and post it, and anyone else can use that template. Perusing Jekyllthemes.org, you can pick out any template you’d like.\n\n\nHow did I get started with Jekyll?\nAs a blogging noob, I’ve discovered there are hundreds of ways to generate a blog (e.g., WordPress, Squarespace, blogger.com, Weebly, etc.). But when my colleague Arthur Lui showed me his blog, I wanted something similar. I first tried Wordpress, then tried blogspot, then gave up and followed Arthur’s example and landed with Jekyll.\n\n\nSet up a blog in 10 minutes!\nI followed this Jekyll tutorial to set up my first blog. I’ll simplify those steps here to get you up and running quickly:\n\nGet a GitHub account. (2 min)\nGo to Jekyllthemes.org. All of these blog themes are free. Find one you like, and Fork the repo (i.e., “repository”). (2-60 min, depending on how long you peruse)\nIn GitHub, rename the newly-forked repo from “REPO name” to “yourgithubusername.github.io”. (1 min)\nNow go to “githubusername.github.io”. Voila, you have a blog.\n\nAnd if you want to start writing posts, go to your “_posts” folder. Then create new files and follow the default templates that the repository provides. (The blogs will need to be written in Markdown syntax.)\nThat was my 10 minute promise. If you feel a bit cheated or lost, go to the well-done tutorial and do all the steps. But the steps I listed are the essential steps to setting up a blog from scratch. So if you want to make a few blogs really quickly, after you figure out the first one the others follow nicely. For example, once I finished the tutorial, I followed those steps above to try two other themes before deciding on the Centrarium theme.\nTo get Centrarium to work, I did have to change some configurations in the “_config.yml”, so there can be some minor debugging depending on the theme you choose. But with a little patience and some Googling, you’ll get it.\n\n\nAdd to Jekyll a custom URL from GoDaddy\nYou’ll notice I have a custom URL and not “bryanwhiting.github.io”. I used GoDaddy to get a domain. (I have no affiliation with GoDaddy.) Here’s how I linked GoDaddy with my GitHub Jekyll blog:\n\nGet a domain.\nIn your Github repo, change (or create) a CNAME file in the main directory. (See my CNAME file for an example). And for the CNAME file contents, just type in the new “www” that you registered through GoDaddy.\nThen, on GoDaddy, go to the section “DNS Management”. Here’s where you tell GoDaddy how to connect your newly purchased “www” with GitHub. Create the following connections:\n[Type, Name, Value, TTL] = [A, @, 192.30.252.153, 600 seconds]\n[Type, Name, Value, TTL] = [A, @, 192.30.252.154, 600 seconds]\n[Type, Name, Value, TTL] = [CNAME, ftp, yourgithubusername.github.io, 600 seconds]\n[Type, Name, Value, TTL] = [CNAME, www, yourgithubusername.github.io, 600 seconds]\n\nIf you have the CNAME file in your GitHub folder, GitHub and GoDaddy will talk and after a few minutes your custom “www” will work. You don’t technically need the two [A] steps, but those connections allow users to type “bryanwhiting.com” instead of “www.bryanwhiting.com”. It can take up to 24 hours before these [A] connections enable, so don’t be worried if the short URL doesn’t work first try. But the full URL should work soon enough.\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2020-12-15-calendaring-todos/index.html",
    "href": "posts/2020-12-15-calendaring-todos/index.html",
    "title": "Get rid of the todo list. Calendar everything.",
    "section": "",
    "text": "I’ve had less meetings and more unstructured time during WFH. It can be isolating, which can slow down productivity.\nHere’s a tip I read today on how to be productive with unstructured time: put your to-do list directly into a calendar.\nThat way, you’ll always have time for the important things. Remove the things that take up your time and aren’t as important. I like it.\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-23-new-ideas-are-distractingly-exciting/index.html",
    "href": "posts/2022-06-23-new-ideas-are-distractingly-exciting/index.html",
    "title": "New ideas are distractingly exciting",
    "section": "",
    "text": "When a new idea comes to my mind, I want to act on it right away. Perhaps it’s an exciting analysis idea, a new business strategy I want to scope out, or a question to ask a team member.\nBut as I look back at my backlog of things to work on, all of these ideas were also equally exciting.\nWhy is it that new ideas have more weight than old ideas?\nThey say that effectiveness is working quickly on tasks. Efficiency is knowing what to work on. Discipline is the ability to overcome the excitement of a new idea and place it in proper context of all ideas. Sanity is actively saying no to many ideas so you don’t get inundated.\nPerhaps a good strategy is to have a “gut check” for ideas. If they don’t pass the highest threshold then let them go.\nProcess:\n\nWhat’s the new idea?\nWhat’s the potential impact? What would I do with the output of this idea?\nHow feasible is it?\n\nStep 3 is tricky because I don’t want a good idea to go to waste just because it’s hard. Judgment is weighing the trade off between impact and feasibility.\nI like Google Forms for logging ideas. It’s easy to save a Google Form URL to the home screen of my iPhone to make logging new ideas easy. Grooming those ideas is important. Letting go of old ideas is essential. If only I could do the essential…\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-11-18-substance-over-style/index.html",
    "href": "posts/2022-11-18-substance-over-style/index.html",
    "title": "Resume Substance over Style",
    "section": "",
    "text": "Earlier in my career I got feedback from a prospective recruiter telling me my resume was “visually imbalanced”. Of course, the recruiter wanted my business.\nBut I went on to get over 70 interviews with that same template. I got a job at FAANG and a top startup using that resume template. I haven’t changed it.\n\nTakeaway\nHow you write your resume and what’s on it will take you farther than how you design it.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2017-02-10-ggplot-plotly/ggplot-plotly.html",
    "href": "posts/2017-02-10-ggplot-plotly/ggplot-plotly.html",
    "title": "The DataViz battle: Plotly vs ggplot2",
    "section": "",
    "text": "R users fall in love with ggplot2, the growing standard for data visualization in R. The ability to quickly vizualize trends, and customize just about anything you’d want, make it a powerful tool. Yet this week, I made a discovery that may reduce how much I used ggplot2. Enter plot_ly().\nFor this post, I assume that you have a working knowledge of the dplyr (or magrittr) and ggplot2 packages. I caveat that this post is backed with only 4-5 hours using plotly(), so some statements here may not be fully vetted.\nPlotly and ggplot2 are inherently for different purposes. plotly allows you to quickly create beautiful, reactive D3 plots that are particularly powerful in websites and dashboards. You can hover your mouse over the plots and see the data values, zoom in and out of specific regions, and capture stills. Here’s a basic histogram:\n\nset.seed(1)\nlibrary(plotly)\nplot_ly(x = ~ rnorm(1000), type = \"histogram\")\n\n\n\n\n\nAfter a brief dabble this week in plotly, I realized quickly the many advantages that plotly has over ggplot2.\nSeveral initial impressions:\n\nPlotly handles multiple wide data columns. I always find it annoying that to color different series in ggplot2, your data had to be in long format. Granted, it takes one simple melt() command to get the data into wide format.\nPlotly also handles long format (see below).\nCustomizing the layout (plot borders, y axis) is easier.\nCustomizing the legend is easier (in ggplot2 I’ve wanted to remove just one series, which isn’t always easy).\nDocumentation is better in Plotly.\nPlotly syntax is very intuitive (learning how aes() in ggplot2 works is tricky at first)\nPlotly also works for Python, Matlab, and Excel, among other languages.\nIt’s very easy to add new series and customize them (one line, one scatter, and one bar, for example)\nYou can use other fonts (which is possible in ggplot2, but I’ve never gotten to work on my Windows machine)\nYou can toggle series on and off by clicking the series name in the legend\n\nBenefits of ggplot2 over plotly:\n\nFacet wrapping is very easy in ggplot2. (I think you have to do subplots in plotly.)\nggplot2 is probably quicker for exploratory analysis.\n\n\nConverting ggplot2 into plotly\nAn additional benefit of plotly is that you can convert your ggplot() graphs into a plotly object.\n\nlibrary(ggplot2)\np &lt;- qplot(x = rnorm(1000), geom = \"histogram\")\np\n\n\n\n\n\n\n\n\nThen, invoking the ggplotly(p) command, we see the transformation:\n\nggplotly(p)\n\n\n\n\n\nA draw back of ggplotly() is that if you do refined customization (like putting your legend on the bottom of the graph), ggplotly() doesn’t seem to pick this up by default.\n\n# ggplot with legend on the bottom\np &lt;- qplot(\n  data = iris,\n  x = Sepal.Width,\n  y = Sepal.Length,\n  geom = \"point\",\n  color = Species\n) +\n  theme(legend.position = \"bottom\")\np\n\n\n\n\n\n\n\n# Plotly doesn't pick up the legend change\nggplotly(p)\n\n\n\n\n\nBut since Plotly also saves to an object, you can use the %&gt;% notation to pipe and add additional plotting commands. This is similar to the + operator in ggplot().\n\np &lt;- qplot(\n  data = iris,\n  x = Sepal.Width,\n  y = Sepal.Length,\n  geom = \"point\",\n  color = Species\n) +\n  theme(legend.position = \"bottom\")\np2 &lt;- ggplotly(p)\n# Use the plotly layout() command for legend customization\np2 %&gt;% layout(legend = list(orientation = \"h\"))\n\n\n\n\n\nThe legend doesn’t do exactly what we want, but you can manipulate the legend location manually using x and y coordinates. The orientation = 'h' setting in the docs puts the legend on the bottom for default plot_ly() objects. Graphing the same series, we see the legend at the bottom:\n\nplot_ly(iris,\n  x = ~Sepal.Width,\n  y = ~Sepal.Length,\n  type = \"scatter\",\n  mode = \"markers\",\n  color = ~Species\n) %&gt;%\n  layout(legend = list(orientation = \"h\"))\n\n\n\n\n\n(You notice the Plotly X-axis title can get cut off1, so let’s put that +1 to ggplot2.)\n1 This may depend on your screen.Plotly seems very intuitive relative to ggplot2 in doing layout customization. Things that took me many iterations on StackOverflow to figure out, like adding a black line on y = 0, are built in to Plotly.\n\np &lt;- plot_ly(iris,\n  x = ~Sepal.Width,\n  y = ~Sepal.Length,\n  type = \"scatter\",\n  mode = \"markers\",\n  color = ~Species\n)\n# Put legend on bottom, change the x-axis range, and turn on the x-axis line. \n# Also, make the zeroline visible, and turn it red.\np &lt;- p %&gt;% layout(\n  legend = list(orientation = \"h\"),\n  xaxis = list(\n    zeroline = T, # Turns x = 0 on\n    zerolinecolor = \"red\", # colors x = 0 red\n    showline = T, # Shows xaxis border line\n    range = c(-2, 7)\n  )\n)\n# Or, save parameters into a list. Use new fonts (a huge plus)\nf1 &lt;- list(\n  family = \"Arial, sans-serif\",\n  size = 18,\n  color = \"lightgrey\"\n)\nyax &lt;- list(\n  title = \"Sepal length\",\n  titlefont = f1\n)\np %&gt;% layout(yaxis = yax)\n\n\n\n\n\nThings I’d like to further explore:\n\nYou can export static plotly images out to file. My hypothesis is that Plotly images take longer to generate than ggplot2. So if I’m mass producing 30,000 plots (which I had to do last month), which is the faster approach? I would assume ggplot2.\n\n\n\nPlotly in RShiny Dashboards\nThe goal in learning Plotly was for me was to eliminate the Excel-VBA dashboard I created using for my manager. Excel has (some) benefits over ggplot2 static charts: you can easily hover your mouse over a series to see the data value, and most industry users know how to manage an Excel axes. Grated, you can build in an RShiny widget to allow the user to control the axes, but Excel comes with that knowledge base built-in. ggvis allows for the powerful library of Google charts, but I think for a reactive dashboard, plotly is a great way to go2.\n2 Of course, this is relative to the chart you’re trying to make.So Plotly solved the Excel problem for me. Now my manager can click and zoom to the parts of the graph that are interesting, and hover the mouse to see the values. Just use renderPlotly() instead of renderPlot() in the server.R file, and plotlyOutput() instead of plotOutput() in the ui.R file.\nMore info here: RShiny and Plotly\n\n\nRShiny vs Plotly Dashboards\nBoth RShiny and Plotly allow for creating dashboards. Plotly allows you to build dashboards as well. If you’re just interested in only visualizing charts and trends, Plotly dashboards seem like the way to go. But to build reactivity into your dashboard (like subsetting your sample, changing date ranges, etc.), RShiny still seems like the more customizable solution.\n\n\nFinal thoughts\nOverall, it seems that ggplot2 is quicker to build and find what you want. With facet wrapping, the qplot() command, and ggsave(), you can whip something up fast. Plotly is better for dashboards, as you can interact with the plots. I feel like Plotly has a better syntax and documentation, and so it may be easier to get a basic plot to look how you want it to. But ggplot2 seems to have more advanced features, so if you want to get into refined customization, you may want to stick with ggplot2. They’re both great, and serve different purposes, but I’ll be using plotly for my RMarkdown and RShiny visualizations going forward.\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2020-12-15-data-beats-opinions/index.html",
    "href": "posts/2020-12-15-data-beats-opinions/index.html",
    "title": "Data Beats Opinions",
    "section": "",
    "text": "When I co-started a company several years ago, my founders and I would canvass neighborhoods knocking doors. We’d discuss our ideas for a new product one door at a time. After each conversation, the product took a new form. Eventually, the product took shape and we had a successful kickstarter launch.\nSince that time, I’ve thought about how valuable it was to get information straight from the source.\nAlberto Savoia’s new book, “The Right It”, is re-teaching me this principle. Data beats opinions, he says. In another book I’m reading, “Trustworthy Online Controlled Experiments”, the former CEO of Netscape is quoted saying,\n\nIf we have data, let’s go with data. If all we have are opinions, let’s go with mine. - Jim Barksdale\n\nData sourced from your manager, your stakeholder, or your customer are invaluable to help you know if you’re solving their problem. Too often it’s easy to get stuck in the original solution that comes to our mind.\nData from customers directly shape the product. Data sourced at scale from online experiments refine it.\nOpinions from the customer shape the product. Their behavior (captured as data) speaks louder than their words. Frequently, it’s challenging to correlate the two. We’re left to guess why someone actually clicked, bought, churned. It’s a beautiful cycle.\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html",
    "title": "Buying a used car the data science way: Part 2",
    "section": "",
    "text": "[Update 2021-11-16] This analysis was originally written on my old blog here. You can find the source code for it here.\nThis is part 2 out of a two-part series on scraping used car data. Check out part 1 to learn how to scrape the data.\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#youre-in-the-market",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#youre-in-the-market",
    "title": "Buying a used car the data science way: Part 2",
    "section": "You’re in the market",
    "text": "You’re in the market\nSo you want to buy a car, but you don’t know anything about them? Welcome to my life.\nYou show up at the dealer and there’s a sticker on the window. You know the difference between make and model, but you soon learn what a trim is. Some versions come with leather. Some have a sun roof. Some have all wheel drive. Some have 20k in miles, and a similarly priced car in a higher trim is at 40k miles. How do you know what you’re really paying for, and what these features are worth? And how do you know what it’ll be worth when it’s four years older and has an additional 40k miles?\nPretty advanced questions for someone who just learned what a powertrain is.\nIn this analysis, I’ll dive into how you can use data to learn a lot about an industry in a short amount of time. I’ll first dive into knowing what you’re buying (now and later). Then, I’ll dive into how Truecar and others might be finding their ‘deals’."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#what-am-i-paying-for",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#what-am-i-paying-for",
    "title": "Buying a used car the data science way: Part 2",
    "section": "What am I paying for?",
    "text": "What am I paying for?\nTo do any analysis, you need data. Please see my most recent post if you’re interested in learning how I got the data for this analysis. Suffice it to say, I webscraped it. The data is not my own, but belongs to Truecar.\nI love using linear regression to understand my data on a granular level. In my data set, I was able to extract the make, model, year, mileage, trim, and a flag for whether the car was all wheel drive (awd == 1) or not. Putting this data into a linear regression, we can see some interesting results. First let’s look at the data we have:\n\nlibrary(dplyr)\nload('../../data/used-car/tc-ford-edge.Rda')\n# Get the prices into the appropriate scale and remove some extraneous\ndf$price = df$price*1000\ndf %&gt;% select(-stats, -url, -int, -vin, -v6, -trueprice, -resid) %&gt;% head()\n\n##   price year mileage trim awd        location                   ext deal\n## 1 16000 2014  28.477   SE   1    Sterling, VA                  &lt;NA&gt; 6514\n## 2 13300 2013  64.315   SE   0   Chantilly, VA                  &lt;NA&gt; 1784\n## 3 13800 2013  72.111  SEL   0     Fairfax, VA Mineral Gray Metallic 3331\n## 4 16000 2014  27.490   SE   0    Manassas, VA          Oxford White 5448\n## 5 12000 2013  66.825   SE   0 Clarksville, MD Mineral Gray Metallic   NA\n## 6 14900 2014  65.157   SE   0    Manassas, VA Tuxedo Black Metallic 4436\n##    mpyr\n## 1  7.12\n## 2 12.86\n## 3 14.42\n## 4  6.87\n## 5 13.37\n## 6 16.29\nNow let’s see the regression results.\n\nmod &lt;- lm(price ~ as.factor(year) + mileage + trim + awd, data = df)\nsummary(mod)\n\n## \n## Call:\n## lm(formula = price ~ as.factor(year) + mileage + trim + awd, \n##     data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4755.7  -854.6    -2.7   877.5  5024.2 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         22216.400    788.885  28.162  &lt; 2e-16 ***\n## as.factor(year)2013  1267.037    625.804   2.025   0.0442 *  \n## as.factor(year)2014  3160.017    645.152   4.898 1.92e-06 ***\n## as.factor(year)2015  5664.470    710.322   7.975 9.56e-14 ***\n## as.factor(year)2016  6166.658    821.469   7.507 1.68e-12 ***\n## as.factor(year)2017  9082.971    878.956  10.334  &lt; 2e-16 ***\n## mileage               -91.951      5.625 -16.347  &lt; 2e-16 ***\n## trimSE              -4983.725    409.765 -12.162  &lt; 2e-16 ***\n## trimSEL             -1822.889    291.400  -6.256 2.17e-09 ***\n## trimSport            3419.270    374.977   9.119  &lt; 2e-16 ***\n## trimTitanium           99.242    568.428   0.175   0.8616    \n## awd                  1277.734    222.680   5.738 3.30e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1450 on 211 degrees of freedom\n## Multiple R-squared:  0.9231, Adjusted R-squared:  0.9191 \n## F-statistic: 230.2 on 11 and 211 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#interpretation-matters",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#interpretation-matters",
    "title": "Buying a used car the data science way: Part 2",
    "section": "Interpretation matters",
    "text": "Interpretation matters\nFirst, we see the (Intercept). This is the average value of a car for every level of every feature not show. So it represents the average price of a year 2000 with 0 miles, Limited trim, without all wheel drive. This allows us to now see what the average effect of, say, year is, holding all else constant. Every unit is in terms of dollars, so we can see that for as.factor(year)2013 that 2013 cars are worth $1,267 more than 2012 cars. This makes sense.\nI scaled mileage to be in the thousands so that it’s a little easier to interpret. So we see that for every 1,000 miles you drive a Ford Edge, it decreases the value of the car by $91 dollars. This tells you a few things. One, if I’m being offered two cars that are 10K miles different, there should be about a ~$900 difference in the car.\nSecond, if I’m planning on driving my car 30k miles over the next two years, how much should I expect the price to drop just due to mileage? Well, about -$91 * 30 = -$2,730. And maybe if you bought a 2014 model, two years later it could perhaps be like owning a 2012 model today. What’s the two-year difference between a 2014 and a 2012 model? About $3,160 dollars."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#inference",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#inference",
    "title": "Buying a used car the data science way: Part 2",
    "section": "Inference",
    "text": "Inference\nNow it’s getting interesting! You’re now starting to remember how your mom told you to never buy a car new, because once you drive it off the lot, it’s worth significantly less. Well, we can see that if you own a brand-new 2017 model and a 2016 model, they’re about $9082 - $6166 = $2916 different. So yea, just by getting the 2017 model you’ll lose $3,000 in the first year. See how 2016 is only $502 dollars more than the 2015? Having the 2015 model is practically the same price as a 2016. So you might as well buy the 2016 model and save a year of wear.\nWell, my theory kind of starts breaking down because 2015 cars and 2014 cars are similarly spaced as 2014 and 2013 ones. So maybe I shouldn’t read too much into this. But the ability to see these trends shouldn’t stop you from asking important questions, like, why is there a difference between years?\nWhen investigating the Nissan Murano, I observed a $5430 difference between 2015 and 2014. That’s non-trivial considering the other years were equally spaced. Doing a little research I saw that Nissan changed the 2015 Murano style, and the new style was apparently worth a lot more. Turns out the same thing happened here for the Ford Edge - the 2015 model is slightly longer with redesigned interior.\nYou can also use this framework to get a decent sense for what features are important to you. Like, is having all wheel drive really worth $1,277? If you live in an area where it snows, it probably is. But if you live in Florida, you may not need it.\nNot only is this framework helpful for comparing within-make-model differences, it helps you get a sense of between-make-model differences. Say, comparing the Ford Edge with the Nissan Murano and the Toyota RAV4. The coefficient for milage on the Edge is -$91. On the Murano it’s -$83, and on the RAV4 -$61. Remember this is the cost per 1,000 miles. That tells me that if I ‘spend’ 30k miles on a car over two years, I’ll lose -$2,730 on the Edge, but only -$1,830 on the RAV4, saving me $900. This also validates that Toyotas hold their value more."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#identify-a-good-market-deal-and-how-truecar-might-do-it",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#identify-a-good-market-deal-and-how-truecar-might-do-it",
    "title": "Buying a used car the data science way: Part 2",
    "section": "Identify a good market deal (and how TrueCar might do it)",
    "text": "Identify a good market deal (and how TrueCar might do it)\nBeyond the research above, you can use such a model to identify deals in the market and guide your buying decision. Here’s how:\n\nMake a PriceFinder: Get the residuals (actual minus predicted value) for each car. The more negative the residual, the more this car is a good price! This may be how TrueCar and CarGurus get their ‘deals’. They fit a model (probably one better than what I’ve created here) and use its residuals to value each car.\nIdentify ‘overpriced’ cars and try to possibly haggle those dealers down. Again, use the residuals here.\nUse this model to predict ‘out-of-sample’ cars. You can use this model built on TrueCar data and find a car on the classifieds or another site and evaluate whether it’s a good deal or not.\n\nHere’s an example of how we do that. First, let’s predict a hypothetical 2015 SEL Ford Edge FWD with 27.6k miles:\n\nthat_car &lt;- as.data.frame(list(\n  year = 2015,\n  mileage = 27.6,\n  trim = 'SEL',\n  awd = 0\n))\n\npredict(mod, newdata = that_car)\n\n##        1 \n## 23520.13\nWe’d expect, on average, such a car to cost $23,520.13. Is what the dealer’s offering you above or below that? If they’re offering $25,000 then you know the car is $1,479.87 higher than what you might expect."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#how-to-improve-our-model",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#how-to-improve-our-model",
    "title": "Buying a used car the data science way: Part 2",
    "section": "How to improve our model",
    "text": "How to improve our model\nNow, this is a pretty naive model, but it’s a model. As George Box said, “All models are wrong, but some are useful.” But here are some quick ways to improve this model:\n\nCheck for interactions. An interaction between miles and year would tell you whether the effect of miles would change across different years. For example, I think adding 10k miles to a 2017 would devalue the car more steeply than adding 10k miles to a 2012 model. During my quick research I checked for interactions, but I didn’t find many to be significant and I chose to ignore them for simplicity.\nUse a linear trend for year rather than a ‘factor’ trend. I’ve chosen here to treat year as a factor, which implies that I think that each year is completely different. Had I used year as a numeric feature, I could have possibly interpreted year as, “Each additional year increases/decreases the price by X on average, holding all else constant.” I liked the simplicity of thinking that each year was different, and since cars usually update models every four years, I wanted to be able to see where this happened. You couldn’t easily see this if year were a linear trend. Objectively, what would be a good way to choose? Fit both models and see which had better R-squared or AIC.\nDo model diagnostics. Check the residual plot to make sure the errors are normally distributed. Check for outliers. Each of these affects the accuracy of your residuals. I did some brief checks and was genearlly satisfied."
  },
  {
    "objectID": "posts/2017-10-19-used-cars-analysis/used-cars.html#takeaways",
    "href": "posts/2017-10-19-used-cars-analysis/used-cars.html#takeaways",
    "title": "Buying a used car the data science way: Part 2",
    "section": "Takeaways",
    "text": "Takeaways\nSo, a data scientist walks into a dealership with a computer with a registry of thousands of used cars on it. He negotiates with the front-office salesman. And the salesman says…\n\nNon-statistical considerations\nWhen you’re buying a car from a dealership, remember that it’s their job to sell you that car. Here are other random things I learned to think about during the car-buying process.\n\nHow many miles has it been driven, and how old is it? (We found a car with 45,000 miles in one year. That’s almost two trips around the entire world in a year! This is also 3750 miles per month (a cross-country US trip), or about 125 miles per day. What kind of driving do you have to do in a year to get that?\nHow many prior owners were there?\nWas the car previously a rental? A lease? A fleet?\nRental: Abused by X number of people? But also perhaps better maintained by rental companies?\nLease: What kind of person would lease?\nFleet: Probably a corporate car. Like a rental, but different kind of drivers.\nWhere did the car come from?\nIs there snow where they live? And do they use salt on the roads there? (Salt = rust underneath)\nWhat else does the geography tell you about the possible wear on the car (causing long-term problems)\nWhat does the CarFax or Autocheck say?\nWas the car sold at auction? When? What could the time between sales tell you about the car?\nIf someone gets into an accident and doesn’t report it, the CarFax won’t tell you. I drove 45 minutes to look at a ‘great’ deal only to learn the door was nearly falling off.\nGood luck haggling the dealer down. Car dealerships don’t usually make that much on the car, but make their money on warranties and fees. Your best chance is to haggle on the deals being offered (terms and agreements).\nDon’t get the powertrain warranty. This is like the “french fries” of the auto industry. Pure profit.\nEvery dealership has quota. Going in at the end of the month might help you, but the dealership will be packed.\nUsed cars are bought at auction. Carmax buys all their cars at auction and only keeps 1/3 of them. Local dealers also get their cars this way, and they don’t always know what the car looks like till it gets to their lot. Check the Carfax to see if any prior dealers ‘flipped’ the car quickly.\n\nUsed cars have a shelf life. If a car isn’t sold in 45-60 days, some dealerships might have to auction off the car. That means you’ll see a tiering in the price. Less-favorable cars will see the price drop several times in its life cycle. Check CarGurus to see how many times the price has dropped, and try to guess for yourself if it might drop again (or just get sold). They know their prices. They’ve got their back-office guys looking at every price of every same car within 100 mile radius. While I’m also able to get as many of those cars as available on TrueCar, I’m not able to segment by all the various features. It’s easy to webscrape for Make, Model, Year, AWD/FWD, and maybe V6 engine, but it’s very hard to get the details like leather, sunroof, twin-turbo, etc. That’s where the pricing gets really interesting.\n\n\nFinal thoughts\nUltimately, a car is bought by irrational humans, and sold by more experienced ones too. So all of this data will probably go out the door when you try to make a deal.\nSo a data scientist walked into several dealerships, and left frustrated because nobody budged on their prices. Doesn’t matter how much data I have, or what I think I know, everyone’s got their price."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html",
    "title": "What is the best todo list manager?",
    "section": "",
    "text": "Wirecutter reviewed the best todo list apps and declared “ticktick” the winner. After all my research here’s what I’ve learned.\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#structured",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#structured",
    "title": "What is the best todo list manager?",
    "section": "Structured",
    "text": "Structured\nThe highest rated app for planning on the App Store is “Structured”. Where this app excels is it’s simplicity. It’s likely best for someone like a student that wants to plan out when they’ll get their homework done. What I loved about this app, and what I wish many others apps had, is a duration estimation for a task. Why is it so hard for developers to realize that tasks take time? They obsess over list keeping and app design but ignore the second most basic question: how long will this take? Structured kind of solves this. What I don’t like about this app is the design. While others praise it for its beauty and simplicity I thought the visual feel of it was distracting."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#microsoft-to-do",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#microsoft-to-do",
    "title": "What is the best todo list manager?",
    "section": "Microsoft To Do",
    "text": "Microsoft To Do\nMicrosoft To Do used to be a favorite of mine. I loved its daily planner and smart suggestions. It has the philosophy that every day you should start fresh. All unfinished tasks will be removed from today and you’re left to pick and choose what you want to do. The fatal limitation however is that it lacks the ability to plan tomorrows a tasks the day before. So this forces people to have to populate their tasks at the start of the day. This is the worst time for planning for me because I want to get up and get going ASAP. (But honestly most times I wake up 15 minutes before my first meeting or am spending time pouring milk into my kids’ cereal.) Morning is not the best time to plan a day for me, so I lost interest here.\n\n\n\nTo Do has smart text parsing.\n\n\n\n\n\nTo Do has “smart suggestions” that you can add to your day."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#todoist",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#todoist",
    "title": "What is the best todo list manager?",
    "section": "Todoist",
    "text": "Todoist\nI’ve tried many times to get into Todoist. It has smart text parsing which is apparently best in class from my Reddit research.\nThings it has:\n\nProjects can have section labels. This lets you organize your list more meaningfully. This is great, only that TickTick does it better. Each project in TickTick can have subprojects. These subprojects act like sections. You can also move subproject from one list to another.\n\n\nThings it lacks:\n\nCan’t nest labels (tags)\nCan’t create labels dynamically - you need to manually add them on the label screen before a task can leverage them.\nCan’t pull in my calendar.\nDoesn’t have check items, only subtasks."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#sorted3",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#sorted3",
    "title": "What is the best todo list manager?",
    "section": "Sorted3",
    "text": "Sorted3\nThis app takes a different design. It’s main selling point is the ability to auto schedule your day based on the tasks you have for that day.\nPros:\n\nRich formatting on description. Supports markdown.\nclean UI. Good shortcuts.\nintegrates with calendar to plan your day.\nEasily plan duration.\n\nReally nice widgets.\n\nCons:\n\nDoesn’t have subtasks, but has check boxes\nNo prioritization or smart filters. Just tags and lists.\nCan’t easily see across lists like you can in TickTick\n\nAuto schedule is nice but could be kind of gimmicky. If you have 8 meetings in a day and five 30 min free blocks but all your tasks for the day are an hour, it would put all your tasks at the end of the day. I found it kind of buggy."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#automated",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#automated",
    "title": "What is the best todo list manager?",
    "section": "Automated",
    "text": "Automated\nMotion became popular as a calendar organizer. Add a task and Motion will figure out when to do that for you. If you change your meeting then Motion will adjust when your tasks will get done. Cons: it’s expensive ($20-$35 per month). It’s mobile app is horrid. It didn’t seem like it offered sub tasks and all that. So you really need to trust the algorithm.\n\nIt gets the notion of task-duration right. But if you want to jot down notes of all the next actions you need to take on a project, your up the creek. I’m also not sure how it handles task dependency - if I put in 5 tasks for a project will it know their order? Not sure. Again I’ve never tried this app so take this all with a grain of salt.\nReclaim is another app claiming to do the same thing and offers a free tier. I did as much as set up my account with this.\nPerhaps if I gave these more attention they’d prove their worth. I’ve tried calendaring my todos in the past and it hasn’t been very effective for me. Perhaps these apps would solve it but I think I need a good mobile app.\nAnyway, you can’t beat free and free is working for me on TickTick right now. Also, my calendar is very open generally so I don’t have the same problems a busy manager might have.\nI think these apps would be best for people who have meeting heavy workflows. It handles the schedules as you need."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#akiflow",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#akiflow",
    "title": "What is the best todo list manager?",
    "section": "Akiflow",
    "text": "Akiflow\nThis app seemed expensive ($15/mo), no mobile app, but has an awesome calendar. The philosophy is you have to assign every task to a date and time to move it out of the inbox. That seemed like a headache to me, and seemed like I’d be constantly managing the inbox. That and I’m cheap and want a mobile app."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#sunsama",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#sunsama",
    "title": "What is the best todo list manager?",
    "section": "Sunsama",
    "text": "Sunsama\n$20 per mo or $192/year. Super fast customer support.\nFeatures: * Mobile app, Mac app. * Duration\nPros: * Channels (lists) * great shortcuts * can align tasks with objectives * more of a mindful daily planner * Integrates with Todoist and other apps. You can pull in all to foist metadata including subtasks and check them off in Sunsama. Additionally, you can add duration and calendar your Todoist tasks. * Can export all the tasks to csv. * Can split calendar events.\nCons: * Widgets are weak * No priority labels * No task tags/labels. (But you can use Todoist for this if you want.)"
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#reclaim.ai",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#reclaim.ai",
    "title": "What is the best todo list manager?",
    "section": "Reclaim.ai",
    "text": "Reclaim.ai\nTask manager isn’t as good as Sunsama. Only $8/mo for pro. Has todoist integration too, but that’s a pro feature.\nThis is one I’ll want to try later. It has the habit tracking and smart flexible calendaring. Not sure how good of a task manager it is. TBD."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#vimcal",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#vimcal",
    "title": "What is the best todo list manager?",
    "section": "Vimcal",
    "text": "Vimcal\nFast calendar. Not for tasks or backlogs it seems though. Didn’t try."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#asana",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#asana",
    "title": "What is the best todo list manager?",
    "section": "Asana",
    "text": "Asana\nOh man, how many times have I downloaded Asana? I started with it in 2014 and thought it was cool. Trying it again in 2022 it had a similar feel.\nAsana is the only task manager app I reviewed that has dependency management. Sure, Jira has blockers and all that but Jiras more aimed toward high level project management. It’s not fast at the\nOther things I remember liking:\n\ninfinite subtasks. This was fun and also distracting.\nAutomation features. You can create workflows for what happens to a task\nCommunication. It has a nice inbox feature that let me communicate with my manager easily.\n\nCons:\n\nClunky on the board management. If I want to start a new project things get messy and overwhelming."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#jiragithub-projects",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#jiragithub-projects",
    "title": "What is the best todo list manager?",
    "section": "Jira/GitHub projects",
    "text": "Jira/GitHub projects\nGreat at a high level but messy to manage the gritty day to day. Doesn’t let you schedule or put due dates. So eventually I’d need some other tool to manage all the minutia of getting things done."
  },
  {
    "objectID": "posts/2022-06-23-the-best-todolist-manager/index.html#not-considered",
    "href": "posts/2022-06-23-the-best-todolist-manager/index.html#not-considered",
    "title": "What is the best todo list manager?",
    "section": "Not considered",
    "text": "Not considered\nI didn’t look at Things3 or OmniFocus, which are considered of the best tools. Things3 was too expensive and seemed like it had many of the same features as TickTick, except I personally didn’t like the UI."
  },
  {
    "objectID": "posts/2023-12-12-future-of-coding/index.html",
    "href": "posts/2023-12-12-future-of-coding/index.html",
    "title": "Future of Coding",
    "section": "",
    "text": "The Essence\n\n\n\n\n\n\nTL;DR: The future belongs to those whom it’s always belonged\n\n\n\n\n\n\nA blogger data scientist Santiago recently said on Twitter, (paraphrasing) “I’m going to focus on what won’t change”. He was worried about how coding and AI will be automated, but Jeff Bezos wasn’t worried about so much of the world changing with the Internet. Bezos focused on what wasn’t changing.\nThis thought hit me, because I think it’s relevant to what our future looks like in 18 months.\n\n\nEvery day, a new winner\nNothings more apparent that the world is going to be wildly different than the past than to hop on X and see new academics one upping themselves with the next greatest LLM. Mistral just announced a 2Bn valuation as a 6 month old company. Fast is the new pace. And it’s oddly the case that big businesses are capable of massive disruption from upstarts. They can’t afford to stay glacial much longer (5-10 years).\nWith all of this, what about this blog post won’t change? Well, the timeless principle is that the nature of the game has changed permanently. Pandora’s box was opened.\n\n\nThe rise of a new equalizer\nGo into a poor part of town and you’re guaranteed to see one thing: poor people using smart phones.\nThey have access to all the same information that the rich people have. The same excellent software. Same excellent hardware. All for a fee of $50/mo for the hardware and $25/mo for an unlimited data plan.\nSo that out the rich and the poor on the same level.\nWell now, knowledge and reasoning is as accessible as Google Maps. You don’t have to pay $20/mo for ChatGPT plus. You can use Bing, Claude.ai, bard.google.com, You.com, or perplexity.ai for free. Now.\nSure people hate on bard and want to use the latest and greatest. But even the crappiest of the four I mentioned above (granted Bing/You.com use GPT in the background), is still leaps ahead of where we were just a year ago. If we were stuck only using one of these tools forever we’d still be massively more productive than not.\nSo knowledge and intellectual reasoning is going to be commonplace amongst the plebes.\n\n\nif you value intelligence above all other human qualities, you’re gonna have a bad time\n\n— Ilya Sutskever (@ilyasut) October 7, 2023\n\n\n\n\nMore of the same\nSo…what’s going to happen? Well, I’m not sure human nature will change much.\nThey say “being rich doesn’t make you happier. It just makes you more of what you already were before being rich”.\nAnd so, the mentally poor will remain so. They’ll use AI for entertainment. Better video games. Better, faster content. More consumption.\nThe creatives will be more creative. Nobody will be at a barrier of being silo’d. So what if you’ve only done data science for 10 years. You know coding. It’s easy to now do web design. Or copy writing. Or art. Or…\nAnd I won’t be able to stop the masses from using Code Interpreter to do data science. The systems will only become more powerful, so cloud companies will make it easier to write sql or whatever. Heck, 5-10 years English will be the only programming language. (Who here does firmware/bit mapping anymore? Only a select few.)\n\n\nThe hottest new programming language is English\n\n— Andrej Karpathy (@karpathy) January 24, 2023\n\n\nPython programmers will become like the engineers who create the seeds. And the rest of us just get bigger, better, and more efficient farms. There will always be those who specialize, as there are people who write compilers today, but fewer and farther between. Perhaps intellectual interest and economic incentives will equalize.\n\n\nFestivus for the rest of us\nCompanies will become smaller. But the same human needs will need to be met:\n\nfood\nshelter\nclothes\nphysical toys\ndigital entertainment\n\nMore people will just be able to do more things. Perhaps there will be more prosperity as a result.\nAnd more people will unionize and try to prevent the change. But that’s a short term thing. Can’t last forever. A union is only as good as it’s needed.\n\n\nTakeaways\nGet ahead by focusing on what’s essential. Leverage what’s around you to develop new skills to prepare for the future. And be excited. It’s going to be a wild ride.\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2020-12-16-pretotyping-ml-solutions/index.html",
    "href": "posts/2020-12-16-pretotyping-ml-solutions/index.html",
    "title": "How to mock the value of an ML solution",
    "section": "",
    "text": "IBM once launched a demo of speech-to-text in the early day’s of the technology. But there was a twist: it was smoke and mirrors. A typist in behind the stage was translating and typing.\nThis is a mechanical Turk.\nIn data science, we can present results or dashboards to audience members before they see anything. If you see a big reaction, you know you found a number they cared about.\nGo ahead, write the conclusion first. Fake the chart first. Then back out the code that could produce the insights they need.\nSometimes, unique insights appear along the way. But most of the time you can properly gut-check whether someone will care about what you’re doing. Save time. Work backward.\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-21-products-for-dads/index.html",
    "href": "posts/2022-06-21-products-for-dads/index.html",
    "title": "Products for dads",
    "section": "",
    "text": "Products I enjoy as a dad\nThird times a charm - I hope by(/if) kid number four I’ll have this all figured out. It’s amazing how I feel so new to it all despite raising two kids in the last five years. In starting fresh a third time I feel inspired to write down some things I’m rediscovering.\nTo anyone getting started on their baby journey, perhaps these suggestions can help. Your baby is definitely different than mine, so your mileage may vary! But since I’m still using this stuff five years later maybe it can help you too. Either way, congrats and good luck!\n\nProducts I still use on baby 3\nSince I’m a mega influencer (on my very established data science blog here), I get paid $500 every time you click a link below. So please, go crazy.\nI’ll also credit my wife for figuring out to buy these products in the first place.\n\nStretchy swaddle, like this. I like stretchy swaddles because they’re forgiving. The cotton muslin swaddles are nice and soft, but if the fabric doesn’t stretch the right way you may have to redo the swaddle.\nOllie swaddle (nighttime). This thing is ridiculously overpriced for a swaddle. $70? Don’t get it. But if you do, I like it because its super easy to put on and very sturdy (hard for baby to break). At month 2 the baby gets strong and the basic swaddles are so easy to break. That and I feel like I’m gonna hurt my kid if I try to make a cotton swaddle unbreakable. Enter Velcro! I can be gentle on the swaddle but have it be unbreakable. There are a lot of random contraption swaddles that tie the baby up like a pretzel. This is your classic burrito. Insert baby, wrap right side, Velcro left side and you’re done.\nUbbi Steel Odor Locking diaperpail. I also hated the thought five years ago of shelling out $70 for a trash can. Five years since and the thing still performs. I know how well it’s working when garbage day rolls around and I open it to change it. Smells like a gentle coastal breeze, maybe. To this day my wife hasn’t changed it more than a time or two because thankfully I have an iron stomach and unfortunately she does not. Point is: it keeps the diaper smell inside (for the most part) and uses normal garbage bags (cost saver).\nBaby K’tan Original Baby Wrap Carrier, Infant and Child Sling. I’ve tried several carriers. Boba wraps take way too much patience to learn how to put on (patience I ran out of quickly as a new dad). This thing is just two circular fabric swaths, which means it takes two seconds to throw on. The convenience is huge - baby crying? Grab the carrier and snuggle up in 10 seconds or less.\nDockATot Deluxe+ Dock. We just got one off Facebook marketplace for $50 for third kid. We got it because let’s be real - sleeping is hard. The hardest thing about sleeping is the uncertainty what’s happening on the other side of the room in the crib. Grunting. Coughing. Is baby waking up? Needs a pacifier? When it’s 5am and you’re on feeding three of the night, being able to just roll over and plop the little one into the co-sleeper and monitor the situation close up is nice.\n4moms rockaRoo Baby Swing. There are a thousand ways to solve the “where do I put my baby while cooking dinner” problem. We’ve liked this one.\n\n\n\nMust Haves\n\nDish soap. Honestly, nothing gets out a blowout stain like dish soap. That and drying in the sun is a miracle worker. (Apparently this is how cloth diaper people do it.)\nApple cider vinegar for diaper rash. Once the little pimples appear, you’ll try anything to get them to go away. We just discovered this hack recently and I was delightfully surprised how easily it worked.\nThis is the right way to carry a car seat. A friend of mine that has three kids didn’t know this was a thing until he saw me do it. It’s a thing. Do it! Your forearm will thank you.\n\n\n\nToddler Time\nDown the road, these products have hit home runs for me.\n\nMunchkin Miracle 360 Trainer Cup. Every time I put this cup together I consider applying to Munchkin for a job to be their sales rep. This company has very clever designs on their products and this one just crushes it for me. Acts like a normal cup. Performs like a sippy. (I’ll admit I’ve taken a few swigs out of this just to understand how it works!)\nContigo Spill-Proof Kids Tumbler. I think we have 80 water bottles in a junk drawer somewhere. Cheap ones. Expensive ones. They all end up scattered in pieces like the zebra herd before the lion that is your innocent child. This cup is no exception: I’ve lost a straw on one already. But! If you want a water bottle you’ll have the problem of spare parts. (Uhh, this isn’t a recommendation so far…) So why this cup? This cup is just good. I don’t know. It’s easy to assemble and clean (wide top). Doesn’t spill. Kids like straws?\n\n\n\nBooks\nNot that you’ll have time to read…\n\nThe Happiest Baby on the Block; Fully Revised and Updated Second Edition: The New Way to Calm Crying and Help Your Newborn Baby Sleep Longer. I’m sure there’s a quick YouTube video showing the 5 ways to sooth a baby. (Here’s an article.) But if you like books this is a good one. TLDR: sucking, swaddle, shushing, swinging, and side are all soothing to a crying baby.\n1-2-3 Magic: Gentle 3-Step Child & Toddler Discipline for Calm, Effective, and Happy Parenting. This is for toddlers, but I’ve learned this technique has been very helpful for giving me sanity in the terrible two phase. The approach doesn’t always work, but I’ve learned when it does, which is important. TLDR: don’t talk (engage in debate) with your toddler or show emotion when they’re misbehaving. Count them when they do bad acts. That’s 1. That’s 2. That’s 3. Break time. The book explains the method. YouTube is probably a friend here too.\n\n\n\nFinal word\nFatherhood has pushed my limits in all the ways. But I thank God every day I’m a dad. I do a nightly gratitude journal of my favorite moments of the day. If I’m paying attention during the day, at night it’s always my kids that have brought me the happiest moments and greatest joys of the day.\nJust remember: the clothes/products/trips are like the tools used to construct a fine painting. At the end of the day, it doesn’t matter what tools you use. Nobody cares and neither will you. It’s the output that matters most. I’ve found by just really paying attention to and getting curious about my kids in any moment helps me see the beauty of it all, while distractions cause me to think I need this or that be a better dad.\n\n\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2022-06-22-downstream-impact/index.html",
    "href": "posts/2022-06-22-downstream-impact/index.html",
    "title": "Downstream Impact",
    "section": "",
    "text": "Every pea is unique. But it’s still a pea.\n\n\nOnline experiments are powerful tools to see the causal impact of a business strategy. Tweak something, see what users do.\nThe limitations of online experiments is that once you manipulate your user experience there’s no going back. Users are forever impacted by your choice.\nIf you give out a credit card with $100, $300, and $500 lines, people are going to spend more on the $500 line. They might also default more because they’re less able to pay back $500 than they are $100. As a result, you may lose these $500 users forever and might impact other aspects of their life, not to mention impact the marketing department’s strategy.\nA group of users is like a single living organism, if you treat it like that. If you only measure one metric (conversion rate) on the entire massive audience, then you’ll optimize for the “average” user.\nBut nobody is average. We are all distinct. We fall into categories, sure. But to really optimize the whole business we need to optimize for pockets of users. Or, we just need to set up a system that’s personalized.\nTakeaway? Personalize your experiments where possible. Consider the ethical and downstream impacts of the experiment.\n\n\n\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2017-10-18-used-cars-scrape/used-cars-scrape.html",
    "href": "posts/2017-10-18-used-cars-scrape/used-cars-scrape.html",
    "title": "Buying a used car the data science way: Part 1",
    "section": "",
    "text": "[Update 2021-11-16] This analysis was originally written on my old blog here. You can find the source code for it here. This code no longer works as TrueCar changed their CSS to make it more difficult to scrape. It’s still possible, but you’d need to build a custom scraper from scratch.\nThis is part 1 out of a two-part series on scraping used car data. Check out part 2 to learn how to analyze the data.\nIn another post, I describe how I use this data that I’ve scraped, but I wanted to provide a more in-depth tutorial for those interested in how I got the data. Note, this data belongs to Truecar, so all uses herein are for personal and academic reasons only.\n_________________________  For coaching on data analytics or machine learning, reach out."
  },
  {
    "objectID": "posts/2017-10-18-used-cars-scrape/used-cars-scrape.html#get-the-data",
    "href": "posts/2017-10-18-used-cars-scrape/used-cars-scrape.html#get-the-data",
    "title": "Buying a used car the data science way: Part 1",
    "section": "Get the data",
    "text": "Get the data\nIn order to do any good analaysis, you first need data. I prefer to have more data than less, where possible. In this case, I don’t have any data, so I use webscraping to get the data. There are much better tutorials on how to scrape data, so I’ll be light. I use R’s rvest package here, which does a decent job.1 Let’s look at Truecar’s Used Car postings2. First I use google to find the search query on Truecar that I like."
  },
  {
    "objectID": "posts/2017-10-18-used-cars-scrape/used-cars-scrape.html#load-packages",
    "href": "posts/2017-10-18-used-cars-scrape/used-cars-scrape.html#load-packages",
    "title": "Buying a used car the data science way: Part 1",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(magrittr)\n# Find the URL of the data you want to scrape\nurl &lt;- 'https://www.truecar.com/used-cars-for-sale/listings/ford/edge/'\nread_html(url)\n\n## {xml_document}\n## &lt;html lang=\"en-US\"&gt;\n## [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset= ...\n## [2] &lt;body&gt;\\n    &lt;!-- Element target for any partner code meant to execut ...\nYou’ll see there’s a head and a body. Our data’s in the body, so let’s use html_nodes() and html_text() to parse out the data we want. I used Selectorgadget to know what HTML classes to search for.\n\nread_html(url) %&gt;% html_nodes('.col-xs-6.col-sm-8.no-left-padding') %&gt;% html_text()\n\n## character(0)\nSo that’s how you get the data on a single page. If you look closer at the URL, you see a lot of helpful things. First, there’s the make, then the model, then the location-zip, then the year-range, and ultimately the trim. This is a very pretty and clean URL. If you click on a few additional pages, you’ll see the URL opens up with ?page=2.\nhttps://www.truecar.com/used-cars-for-sale/listings/ford/edge/location-90210/?page=2\nThis is our ‘in’ to scraping multiple pages. I won’t bore you with the details of how to get that data into a neat matrix for us to analyze, but suffice it to say that I’m able to do it. Just build a function to construct a URL, and build a loop to go through the different pages, then use lots of str_extract() from the stringr package and gsub to clean up the data.\n\nlibrary(stringr)\n\nmake = 'ford'\nmodel = 'edge'\nzip = '90210'\nyear = 2012\nnpages = 5\n\nurl &lt;- paste('https://www.truecar.com/used-cars-for-sale/listings/', \n             make, '/', \n             model ,\n             '/location-', zip,\n             '/year-',year,'-max/?page=', sep = \"\")\n\nurls &lt;- paste(url, 1:npages, sep = \"\")\n\nscrape &lt;- function(pageno){\n  try(\n    read_html(urls[pageno]) %&gt;% html_nodes('.col-xs-6.col-sm-8.no-left-padding') %&gt;% html_text()\n  )\n}\n\nlong_list = scrape(1)\nfor(i in 2:npages){\n  print(i)\n  new_list = try(scrape(i))\n  \n  error = (\"try-error\" %in% class(new_list))\n  \n  if( error == FALSE ){\n    long_list = c(long_list, new_list) \n  } else {\n    break\n  }\n}\n## [1] 2\nstats &lt;- long_list\ndf &lt;- as.data.frame(stats)\ndf$stats %&lt;&gt;% as.character()\ndf$price &lt;- str_extract(df$stats, '\\\\$[0-9]*,[0-9]*') %&gt;% \n  gsub('Price: |\\\\$|,', '', .) %&gt;%\n  as.numeric()\ndf$year &lt;- str_extract(df$stats, '^[0-9]* ') %&gt;% \n  as.numeric()\ndf$mileage &lt;- str_extract(df$stats, 'Mileage: [0-9]*,[0-9]*') %&gt;% \n  gsub('Mileage: |,', '', .) %&gt;%\n  as.numeric()\n\n# a = df$stats[1]\ndf$trim &lt;- str_extract(df$stats, '.*Mileage:') %&gt;% \n  gsub('FWD|AWD|4x[24]|[24]WD|V6|4-cyl|^[0-9][0-9][0-9][0-9]|4dr|Automatic|Manual|Mileage:', '', ., ignore.case = T) %&gt;% \n  gsub(make, '', ., ignore.case = T) %&gt;% \n  gsub(model, '', ., ignore.case = T) %&gt;% \n  trimws() \n\n\ndf$awd &lt;- grepl('AWD|4WD|4x4', df$stats, ignore.case = T) %&gt;% as.numeric()\ndf$manual &lt;- grepl('manual', df$stats) %&gt;% as.numeric()\ndf$v6 &lt;- grepl('V6', df$stats) %&gt;% as.numeric()\ndf$location &lt;- str_extract(df$stats, 'Location: .*Exterior:') %&gt;% \n  gsub('Location: |Exterior:', '', .) %&gt;% \n  trimws() \ndf$ext &lt;- str_extract(df$stats, 'Exterior: .*Interior:') %&gt;% \n  gsub('Interior:|Exterior:', '', .) %&gt;% \n  trimws() \ndf$int &lt;- str_extract(df$stats, 'Interior: .*VIN:') %&gt;% \n  gsub('Interior: |VIN:', '', .) %&gt;% \n  trimws() \ndf$vin &lt;- str_extract(df$stats, 'VIN: .*\\\\$') %&gt;% \n  gsub('VIN: |\\\\$', '', .) %&gt;% \n  substr(., 1, 17)\ndf$deal &lt;- str_extract(df$stats, '\\\\$[0-9]*,[0-9]* below') %&gt;% \n  gsub('below|\\\\$|,', '', .) %&gt;% trimws() %&gt;%\n  as.numeric()\n\nAnd here’s what the results look like. You’ve got the original scraped data in the stats column and then everything else that you can parse out.\n\n# df was the dataframe object we needed\ndf %&gt;% select(-stats) %&gt;% head(10) %&gt;% formattable::formattable()"
  }
]